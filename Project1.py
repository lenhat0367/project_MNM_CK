from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class ChototScraper:
    def __init__(self, headless=True):
        """Khá»Ÿi táº¡o scraper"""
        gecko_path = "D:/project_MNM_CK/geckodriver.exe"
        firefox_path = "C:/Program Files/Mozilla Firefox/firefox.exe"
        
        self.options = Options()
        self.options.binary_location = firefox_path
        self.options.page_load_strategy = 'eager'
        
        if headless:
            self.options.add_argument('--headless')
        
        self.options.set_preference('general.useragent.override', 
                                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/120.0')
        
        service = Service(executable_path=gecko_path)
        self.driver = webdriver.Firefox(service=service, options=self.options)
        self.driver.set_page_load_timeout(20)
        self.wait = None
        self.data = []
        
        # DANH SÃCH Cá»˜T
        self.required_columns = [
            'URL', 'TÃªn sáº£n pháº©m', 'GiÃ¡', 'TÃªn ngÆ°á»i Ä‘Äƒng', 'Äá»‹a chá»‰', 'Thá»i gian Ä‘Äƒng', 'URL hÃ¬nh áº£nh',
            'Sá»‘ Km Ä‘Ã£ Ä‘i', 'Sá»‘ Ä‘á»i chá»§', 'CÃ³ phá»¥ kiá»‡n Ä‘i kÃ¨m', 'CÃ²n háº¡n Ä‘Äƒng kiá»ƒm',
            'Xuáº¥t xá»©', 'TÃ¬nh tráº¡ng', 'ChÃ­nh sÃ¡ch báº£o hÃ nh',
            'HÃ£ng xe', 'DÃ²ng xe', 'NÄƒm sáº£n xuáº¥t', 'Há»™p sá»‘',
            'NhiÃªn liá»‡u', 'Kiá»ƒu dÃ¡ng', 'Sá»‘ chá»—', 
            'Trá»ng lÆ°á»£ng', 'Trá»ng táº£i'
        ]
        
        # MAPPING ITEMPROP
        self.itemprop_mappings = {
            'mileage_v2': 'Sá»‘ Km Ä‘Ã£ Ä‘i',
            'number_of_owners': 'Sá»‘ Ä‘á»i chá»§',
            'include_accessories': 'CÃ³ phá»¥ kiá»‡n Ä‘i kÃ¨m',
            'valid_registration': 'CÃ²n háº¡n Ä‘Äƒng kiá»ƒm',
            'carorigin': 'Xuáº¥t xá»©',
            'condition_ad': 'TÃ¬nh tráº¡ng',
            'veh_warranty_policy': 'ChÃ­nh sÃ¡ch báº£o hÃ nh',
            'carbrand': 'HÃ£ng xe',
            'carmodel': 'DÃ²ng xe',
            'mfdate': 'NÄƒm sáº£n xuáº¥t',
            'gearbox': 'Há»™p sá»‘',
            'fuel': 'NhiÃªn liá»‡u',
            'cartype': 'Kiá»ƒu dÃ¡ng',
            'carseats': 'Sá»‘ chá»—',
            'veh_unladen_weight': 'Trá»ng lÆ°á»£ng',
            'veh_gross_weight': 'Trá»ng táº£i'
        }
        
    def get_product_links_from_page(self):
        """Láº¥y links sáº£n pháº©m"""
        product_links = []
        print(f"ğŸ“¥ Äang scroll...")
        
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scrolls = 10
        
        while scroll_attempts < max_scrolls:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts += 1
            if new_height == last_height:
                break
            last_height = new_height
        
        print(f"âœ“ ÄÃ£ scroll {scroll_attempts} láº§n")
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            if href:
                if href.startswith('/'):
                    full_url = 'https://xe.chotot.com' + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                if ('chotot.com' in full_url and 
                    re.search(r'/\d+', full_url) and
                    '/mua-ban' in full_url and
                    '?' not in full_url and
                    full_url not in product_links):
                    product_links.append(full_url)
        
        print(f"âœ“ TÃ¬m tháº¥y {len(product_links)} sáº£n pháº©m")
        return product_links
    
    def extract_specs_by_itemprop(self, soup):
        """CÃ o theo itemprop"""
        specs = {}
        
        print(f"    ğŸ”‘ TÃ¬m theo itemprop...")
        
        for prop, label in self.itemprop_mappings.items():
            elem = soup.find(itemprop=prop)
            if elem:
                value = elem.get_text(strip=True)
                if value:
                    specs[label] = value
                    print(f"       âœ“ {label}: {value}")
        
        print(f"    âœ… TÃ¬m Ä‘Æ°á»£c {len(specs)} thÃ´ng sá»‘")
        return specs
    
    def extract_seller_info(self, soup):
        """CÃ o thÃ´ng tin ngÆ°á»i bÃ¡n - VERSION 8.1 FIX HOÃ€N TOÃ€N"""
        seller_info = {
            'TÃªn ngÆ°á»i Ä‘Äƒng': '',
            'Äá»‹a chá»‰': '',
            'Thá»i gian Ä‘Äƒng': '',
            'URL hÃ¬nh áº£nh': ''
        }
        
        try:
            # ===== FIX TÃŠN NGÆ¯á»œI ÄÄ‚NG - Láº¤Y Cáº¢ Cá»¬A HÃ€NG VÃ€ CÃ NHÃ‚N =====
            # TÃ¬m div chá»©a thÃ´ng tin seller
            seller_container = soup.find('div', itemprop='seller')
            
            if seller_container:
                # Method 1: TÃ¬m trong div class="pf9ruvz" (chá»©a tÃªn)
                pf9ruvz_div = seller_container.find('div', class_=re.compile(r'pf9ruvz'))
                
                if pf9ruvz_div:
                    # TÃ¬m tháº» <a> vá»›i href chá»©a "/cua-hang/" HOáº¶C "/user/"
                    seller_link = pf9ruvz_div.find('a', href=re.compile(r'/(cua-hang|user)/'))
                    
                    if seller_link:
                        seller_b = seller_link.find('b')
                        if seller_b:
                            seller_name = seller_b.get_text(strip=True)
                            
                            # Lá»c: khÃ´ng pháº£i sá»‘ rating vÃ  Ä‘á»§ dÃ i
                            if seller_name and not seller_name.replace('.', '').replace('(', '').replace(')', '').isdigit():
                                if len(seller_name) > 1:  # TÃªn Ã­t nháº¥t 2 kÃ½ tá»±
                                    seller_info['TÃªn ngÆ°á»i Ä‘Äƒng'] = seller_name
                                    print(f"       âœ“ TÃªn ngÆ°á»i Ä‘Äƒng: {seller_name}")
            
            # Backup method: TÃ¬m táº¥t cáº£ <b> trong itemprop="seller"
            if not seller_info['TÃªn ngÆ°á»i Ä‘Äƒng'] and seller_container:
                all_b_tags = seller_container.find_all('b')
                for b_tag in all_b_tags:
                    # Kiá»ƒm tra tháº» b nÃ y cÃ³ náº±m trong link /cua-hang/ hoáº·c /user/ khÃ´ng
                    parent_a = b_tag.find_parent('a')
                    if parent_a:
                        href = parent_a.get('href', '')
                        if '/cua-hang/' in href or '/user/' in href:
                            name = b_tag.get_text(strip=True)
                            
                            # Loáº¡i bá» rating sá»‘, text ngáº¯n, text cÃ³ "bÃ¡n"/"ÄÃ¡nh giÃ¡"
                            if name and len(name) > 2:
                                # Kiá»ƒm tra khÃ´ng pháº£i sá»‘ (rating)
                                if not name.replace('.', '').replace('(', '').replace(')', '').isdigit():
                                    # Kiá»ƒm tra khÃ´ng chá»©a tá»« khÃ´ng mong muá»‘n
                                    if not any(x in name.lower() for x in ['bÃ¡n', 'Ä‘Ã¡nh giÃ¡', 'rating']):
                                        seller_info['TÃªn ngÆ°á»i Ä‘Äƒng'] = name
                                        print(f"       âœ“ TÃªn ngÆ°á»i Ä‘Äƒng (backup): {name}")
                                        break
            
            # ===== THá»œI GIAN ÄÄ‚NG BÃ€I =====
            # Pattern: "ÄÄƒng X ngÃ y trÆ°á»›c" hoáº·c "ÄÄƒng X giá» trÆ°á»›c" hoáº·c "ÄÄƒng X phÃºt trÆ°á»›c"
            time_posted = soup.find('span', class_='bwq0cbs', string=re.compile(r'ÄÄƒng.*trÆ°á»›c'))
            if time_posted:
                seller_info['Thá»i gian Ä‘Äƒng'] = time_posted.get_text(strip=True)
                print(f"       âœ“ Thá»i gian Ä‘Äƒng: {seller_info['Thá»i gian Ä‘Äƒng']}")
            else:
                # Backup: TÃ¬m trong táº¥t cáº£ span
                all_spans = soup.find_all('span')
                for span in all_spans:
                    text = span.get_text(strip=True)
                    if 'ÄÄƒng' in text and 'trÆ°á»›c' in text:
                        seller_info['Thá»i gian Ä‘Äƒng'] = text
                        print(f"       âœ“ Thá»i gian Ä‘Äƒng (backup): {text}")
                        break
            
            # ===== Äá»ŠA CHá»ˆ =====
            address_spans = soup.find_all('span', class_='bwq0cbs')
            for span in address_spans:
                text = span.get_text(strip=True)
                # Äá»‹a chá»‰ thÆ°á»ng dÃ i vÃ  chá»©a Ä‘á»‹a danh
                if len(text) > 15 and any(x in text for x in ['PhÆ°á»ng', 'Quáº­n', 'TP', 'Tp', 'Huyá»‡n', 'ThÃ nh phá»‘', 'Tá»‰nh']):
                    # KhÃ´ng láº¥y text cÃ³ "ÄÄƒng", "Ä‘Ã£ bÃ¡n", "Ä‘ang bÃ¡n"
                    if 'ÄÄƒng' not in text and 'bÃ¡n' not in text.lower() and 'Pháº£n há»“i' not in text:
                        seller_info['Äá»‹a chá»‰'] = text
                        print(f"       âœ“ Äá»‹a chá»‰: {text}")
                        break
            
            # ===== URL HÃŒNH áº¢NH =====
            all_imgs = soup.find_all('img', src=True)
            candidate_images = []
            
            for img in all_imgs:
                src = img.get('src', '')
                if not src or any(x in src.lower() for x in ['icon', 'logo', 'static', 'svg']):
                    continue
                
                # Æ¯u tiÃªn áº£nh tá»« CDN
                if 'cdn.chotot.com' in src or 'storage' in src or 'img' in src:
                    width = img.get('width', '')
                    height = img.get('height', '')
                    
                    size = 0
                    if width and str(width).isdigit():
                        size += int(width)
                    if height and str(height).isdigit():
                        size += int(height)
                    
                    candidate_images.append((src, size))
            
            if candidate_images:
                candidate_images.sort(key=lambda x: x[1], reverse=True)
                seller_info['URL hÃ¬nh áº£nh'] = candidate_images[0][0]
                print(f"       âœ“ URL hÃ¬nh áº£nh: {seller_info['URL hÃ¬nh áº£nh'][:60]}...")
            
            if not seller_info['URL hÃ¬nh áº£nh']:
                for img in all_imgs:
                    src = img.get('src', '')
                    if src and src.startswith('http'):
                        seller_info['URL hÃ¬nh áº£nh'] = src
                        print(f"       âœ“ URL hÃ¬nh áº£nh (backup): {src[:60]}...")
                        break
                        
        except Exception as e:
            print(f"       âš ï¸ Lá»—i láº¥y thÃ´ng tin ngÆ°á»i bÃ¡n: {str(e)}")
        
        return seller_info
    
    def scrape_product(self, url):
        """CÃ o thÃ´ng tin chi tiáº¿t"""
        try:
            self.driver.get(url)
            time.sleep(4)
            
            for i in range(3):
                self.driver.execute_script(f"window.scrollTo(0, {(i+1)*800});")
                time.sleep(1.5)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            product_data = {'URL': url, 'TÃªn sáº£n pháº©m': '', 'GiÃ¡': ''}
            
            # Láº¥y tiÃªu Ä‘á»
            title_elem = soup.find('h1')
            if title_elem:
                product_data['TÃªn sáº£n pháº©m'] = title_elem.get_text(strip=True)
            
            # Láº¥y giÃ¡
            price_elem = soup.find('b', class_='p26z2wb')
            if price_elem:
                product_data['GiÃ¡'] = price_elem.get_text(strip=True)
            else:
                price_patterns = [
                    soup.find(string=re.compile(r'\d+\.\d+\.\d+ Ä‘')),
                    soup.find(string=re.compile(r'\d+ triá»‡u')),
                    soup.find(string=re.compile(r'\d+\.\d+ tá»·')),
                ]
                for p in price_patterns:
                    if p:
                        product_data['GiÃ¡'] = p.strip()
                        break
            
            # Láº¥y thÃ´ng tin ngÆ°á»i bÃ¡n, Ä‘á»‹a chá»‰, thá»i gian, hÃ¬nh áº£nh
            seller_info = self.extract_seller_info(soup)
            product_data.update(seller_info)
            
            # Láº¥y thÃ´ng sá»‘ ká»¹ thuáº­t
            specs = self.extract_specs_by_itemprop(soup)
            product_data.update(specs)
            
            return product_data
            
        except Exception as e:
            print(f"    âœ— Lá»—i: {str(e)}")
            return None
    
    def go_to_next_page_direct(self, next_page):
        """Chuyá»ƒn trang"""
        try:
            new_url = f"https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh?page={next_page}"
            print(f"\nâ¡ï¸  Chuyá»ƒn sang trang {next_page}...")
            self.driver.get(new_url)
            time.sleep(4)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            products = soup.find_all('a', href=re.compile(r'/mua-ban.*\d+'))
            
            return len(products) > 0
                
        except Exception as e:
            print(f"\nâœ— Lá»—i: {str(e)}")
            return False
    
    def scrape_test_pages(self, start_url, num_pages=2):
        """CÃ€O TEST"""
        print("=" * 70)
        print("ğŸ”§ Chá»£ Tá»‘t Scraper - Version 8.1 FINAL")
        print("=" * 70)
        print(f"ğŸ”— URL: {start_url}\n")
        
        self.driver.get(start_url)
        time.sleep(3)
        print(f"âœ“ ÄÃ£ vÃ o: {self.driver.title}\n")
        
        for page_num in range(1, num_pages + 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“„ TRANG {page_num}/{num_pages}")
            print(f"{'='*70}")
            
            if page_num > 1:
                if not self.go_to_next_page_direct(page_num):
                    break
            
            product_links = self.get_product_links_from_page()
            
            if not product_links:
                print(f"âš  Trang trá»‘ng!")
                break
            
            print(f"\nğŸ”„ CÃ o {len(product_links)} sáº£n pháº©m...\n")
            
            for i, link in enumerate(product_links, 1):
                print(f"{'='*50}")
                print(f"  [{page_num}.{i}/{len(product_links)}]")
                print(f"{'='*50}")
                
                product_data = self.scrape_product(link)
                
                if product_data and product_data.get('TÃªn sáº£n pháº©m'):
                    self.data.append(product_data)
                    print(f"  âœ… {product_data.get('TÃªn sáº£n pháº©m', '')[:50]}")
                    print(f"     ğŸ’° {product_data.get('GiÃ¡', 'N/A')}")
                    print(f"     ğŸ‘¤ {product_data.get('TÃªn ngÆ°á»i Ä‘Äƒng', 'N/A')}")
                    print(f"     â° {product_data.get('Thá»i gian Ä‘Äƒng', 'N/A')}")
                    
                    spec_count = len([k for k in product_data.keys() if k not in ['URL', 'TÃªn sáº£n pháº©m', 'GiÃ¡', 'TÃªn ngÆ°á»i Ä‘Äƒng', 'Äá»‹a chá»‰', 'Thá»i gian Ä‘Äƒng', 'URL hÃ¬nh áº£nh']])
                    print(f"     ğŸ“Š {spec_count} thÃ´ng sá»‘")
                else:
                    print(f"  âœ— Lá»—i")
                
                time.sleep(1)
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ HOÃ€N Táº¤T: {len(self.data)} sáº£n pháº©m")
        print(f"{'='*70}")
    
    def export_to_excel(self, filename='chotot_final_v8.xlsx'):
        """Xuáº¥t Excel vá»›i thá»‘ng kÃª chi tiáº¿t"""
        if not self.data:
            print("\nâœ— KhÃ´ng cÃ³ dá»¯ liá»‡u!")
            return
        
        df = pd.DataFrame(self.data)
        
        for col in self.required_columns:
            if col not in df.columns:
                df[col] = ''
        
        df = df[self.required_columns]
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"\n{'='*70}")
        print(f"ğŸ’¾ ÄÃƒ LÆ¯U: {filename}")
        print(f"ğŸ“Š {len(self.data)} sáº£n pháº©m Ã— {len(self.required_columns)} cá»™t")
        print(f"{'='*70}")
        
        print(f"\nğŸ“Š THá»NG KÃŠ:")
        
        print(f"\n   ğŸ‘¤ THÃ”NG TIN NGÆ¯á»œI BÃN:")
        seller_fields = ['TÃªn ngÆ°á»i Ä‘Äƒng', 'Äá»‹a chá»‰', 'Thá»i gian Ä‘Äƒng', 'URL hÃ¬nh áº£nh']
        
        for col in seller_fields:
            if col in df.columns:
                non_empty = df[col].astype(str).str.strip().ne('').sum()
                pct = non_empty*100//len(df) if len(df) > 0 else 0
                status = "âœ…" if pct >= 50 else "âš ï¸"
                print(f"      {status} {col}: {non_empty}/{len(df)} ({pct}%)")
        
        print(f"\n   ğŸš— THÃ”NG Sá» XE:")
        spec_fields = ['HÃ£ng xe', 'DÃ²ng xe', 'NÄƒm sáº£n xuáº¥t', 'Há»™p sá»‘', 'NhiÃªn liá»‡u', 'Sá»‘ Km Ä‘Ã£ Ä‘i']
        
        for col in spec_fields:
            if col in df.columns:
                non_empty = df[col].astype(str).str.strip().ne('').sum()
                pct = non_empty*100//len(df) if len(df) > 0 else 0
                status = "âœ…" if pct > 50 else "âš ï¸"
                print(f"      {status} {col}: {non_empty}/{len(df)} ({pct}%)")
        
        print(f"\n{'='*70}")
    
    def close(self):
        """ÄÃ³ng browser"""
        self.driver.quit()


def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ Chá»£ Tá»‘t Scraper - Version 8.1 FINAL")
    print("=" * 70)
    print("ğŸ“‹ Fix HOÃ€N TOÃ€N:")
    print("   âœ“ Láº¥y tÃªn Cáº¢ cá»­a hÃ ng (/cua-hang/) VÃ€ cÃ¡ nhÃ¢n (/user/)")
    print("   âœ“ TÃ¬m trong <div itemprop='seller'>")
    print("   âœ“ Lá»c cháº·t rating sá»‘ vÃ  text khÃ´ng mong muá»‘n")
    print("   âœ“ ThÃªm cá»™t 'Thá»i gian Ä‘Äƒng'")
    print("=" * 70)
    print()
    
    url = "https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh"
    scraper = ChototScraper(headless=True)
    
    try:
        scraper.scrape_test_pages(url, num_pages=2)
        scraper.export_to_excel('chotot_final_v8.1.xlsx')
        
    except KeyboardInterrupt:
        print("\n\nâš  Dá»«ng")
        if scraper.data:
            scraper.export_to_excel('chotot_interrupted_v8.1.xlsx')
    except Exception as e:
        print(f"\nâœ— Lá»—i: {str(e)}")
        import traceback
        traceback.print_exc()
        if scraper.data:
            scraper.export_to_excel('chotot_error_v8.1.xlsx')
    finally:
        print("\nğŸ”’ ÄÃ³ng browser...")
        scraper.close()
        print("âœ… HoÃ n táº¥t!")


if __name__ == "__main__":
    main()