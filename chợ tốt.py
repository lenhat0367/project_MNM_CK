"""
Web Scraper cho Chá»£ Tá»‘t - FIX HOÃ€N TOÃ€N báº±ng itemprop
Version 5: Æ¯u tiÃªn itemprop trÆ°á»›c, backup báº±ng DOM parsing
"""

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class ChototScraper:
    def __init__(self, headless=True):
        """Khá»Ÿi táº¡o scraper"""
        gecko_path = "/Users/binh/thuc_hanh_ma_nguon_mo/gecko bÃ i táº­p /bÃ i táº­p trÃªn lá»›p/geckodriver"
        firefox_path = "/Applications/Firefox.app/Contents/MacOS/firefox"
        
        self.options = Options()
        self.options.binary_location = firefox_path
        self.options.page_load_strategy = 'eager'
        
        if headless:
            self.options.add_argument('--headless')
        
        self.options.set_preference('permissions.default.image', 2)
        self.options.set_preference('general.useragent.override', 
                                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/120.0')
        
        service = Service(executable_path=gecko_path)
        self.driver = webdriver.Firefox(service=service, options=self.options)
        self.driver.set_page_load_timeout(20)
        self.wait = None
        self.data = []
        
        # DANH SÃCH Cá»˜T
        self.required_columns = [
            'URL', 'TiÃªu Ä‘á»', 'GiÃ¡',
            'Sá»‘ Km Ä‘Ã£ Ä‘i', 'Sá»‘ Ä‘á»i chá»§', 'CÃ³ phá»¥ kiá»‡n Ä‘i kÃ¨m', 'CÃ²n háº¡n Ä‘Äƒng kiá»ƒm',
            'Xuáº¥t xá»©', 'TÃ¬nh tráº¡ng', 'ChÃ­nh sÃ¡ch báº£o hÃ nh',
            'HÃ£ng xe', 'DÃ²ng xe', 'NÄƒm sáº£n xuáº¥t', 'PhiÃªn báº£n xe', 'Há»™p sá»‘',
            'NhiÃªn liá»‡u', 'Kiá»ƒu dÃ¡ng', 'Sá»‘ chá»—', 'Há»‡ dáº«n Ä‘á»™ng', 'CÃ´ng suáº¥t Ä‘á»™ng cÆ¡',
            'Momen xoáº¯n', 'Dung tÃ­ch Ä‘á»™ng cÆ¡', 'NhiÃªn liá»‡u tiÃªu thá»¥', 'Sá»‘ tÃºi khÃ­',
            'Khoáº£ng sÃ¡ng gáº§m xe', 'Sá»‘ cá»­a', 'Trá»ng lÆ°á»£ng', 'Trá»ng táº£i'
        ]
        
        # ğŸ”‘ MAPPING ITEMPROP - PHÆ¯Æ NG PHÃP CHÃNH XÃC NHáº¤T
        self.itemprop_mappings = {
            'mileage_v2': 'Sá»‘ Km Ä‘Ã£ Ä‘i',
            'number_of_owners': 'Sá»‘ Ä‘á»i chá»§',
            'include_accessories': 'CÃ³ phá»¥ kiá»‡n Ä‘i kÃ¨m',
            'valid_registration': 'CÃ²n háº¡n Ä‘Äƒng kiá»ƒm',
            'carorigin': 'Xuáº¥t xá»©',
            'condition_ad': 'TÃ¬nh tráº¡ng',
            'veh_warranty_policy': 'ChÃ­nh sÃ¡ch báº£o hÃ nh',
            'carbrand': 'HÃ£ng xe',
            'carmodel': 'DÃ²ng xe',
            'mfdate': 'NÄƒm sáº£n xuáº¥t',
            'option': 'PhiÃªn báº£n xe',
            'gearbox': 'Há»™p sá»‘',
            'fuel': 'NhiÃªn liá»‡u',
            'cartype': 'Kiá»ƒu dÃ¡ng',
            'carseats': 'Sá»‘ chá»—',
            'drivetrain': 'Há»‡ dáº«n Ä‘á»™ng',          # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'horse_power': 'CÃ´ng suáº¥t Ä‘á»™ng cÆ¡',   # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'torque': 'Momen xoáº¯n',               # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'engine_capacity': 'Dung tÃ­ch Ä‘á»™ng cÆ¡', # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'kml_combined': 'NhiÃªn liá»‡u tiÃªu thá»¥', # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'air_bag': 'Sá»‘ tÃºi khÃ­',              # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'minimum_ground_clearance': 'Khoáº£ng sÃ¡ng gáº§m xe',
            'doors': 'Sá»‘ cá»­a',                    # â­ TrÆ°á»ng bá»‹ thiáº¿u
            'veh_unladen_weight': 'Trá»ng lÆ°á»£ng',
            'veh_gross_weight': 'Trá»ng táº£i'
        }
        
    def get_product_links_from_page(self):
        """Láº¥y links sáº£n pháº©m"""
        product_links = []
        print(f"ğŸ“¥ Äang scroll...")
        
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scrolls = 10
        
        while scroll_attempts < max_scrolls:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts += 1
            if new_height == last_height:
                break
            last_height = new_height
        
        print(f"âœ“ ÄÃ£ scroll {scroll_attempts} láº§n")
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            if href:
                if href.startswith('/'):
                    full_url = 'https://xe.chotot.com' + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                if ('chotot.com' in full_url and 
                    re.search(r'/\d+', full_url) and
                    '/mua-ban' in full_url and
                    '?' not in full_url and
                    full_url not in product_links):
                    product_links.append(full_url)
        
        print(f"âœ“ TÃ¬m tháº¥y {len(product_links)} sáº£n pháº©m")
        return product_links
    
    def extract_specs_by_itemprop(self, soup):
        """
        ğŸ”‘ PHÆ¯Æ NG PHÃP CHÃNH: DÃ¹ng itemprop
        ÄÃ¢y lÃ  cÃ¡ch chÃ­nh xÃ¡c 100% vÃ¬ HTML cÃ³ sáºµn attribute itemprop
        """
        specs = {}
        
        print(f"    ğŸ”‘ PhÆ°Æ¡ng phÃ¡p 1: TÃ¬m theo itemprop...")
        
        for prop, label in self.itemprop_mappings.items():
            elem = soup.find(itemprop=prop)
            if elem:
                value = elem.get_text(strip=True)
                if value:
                    specs[label] = value
                    print(f"       âœ“ {label}: {value}")
        
        print(f"    âœ… TÃ¬m Ä‘Æ°á»£c {len(specs)} thÃ´ng sá»‘ qua itemprop")
        return specs
    
    def extract_specs_by_label_matching(self, soup):
        """
        ğŸ”„ PHÆ¯Æ NG PHÃP BACKUP: TÃ¬m theo label text
        TÃ¬m táº¥t cáº£ span cÃ³ text lÃ  label, láº¥y span káº¿ tiáº¿p lÃ m value
        """
        specs = {}
        
        print(f"    ğŸ”„ PhÆ°Æ¡ng phÃ¡p 2: TÃ¬m theo label matching...")
        
        # Map label text -> column name
        label_map = {
            'Há»‡ dáº«n Ä‘á»™ng': 'Há»‡ dáº«n Ä‘á»™ng',
            'CÃ´ng suáº¥t Ä‘á»™ng cÆ¡': 'CÃ´ng suáº¥t Ä‘á»™ng cÆ¡',
            'Momen xoáº¯n': 'Momen xoáº¯n',
            'Dung tÃ­ch Ä‘á»™ng cÆ¡': 'Dung tÃ­ch Ä‘á»™ng cÆ¡',
            'NhiÃªn liá»‡u tiÃªu thá»¥': 'NhiÃªn liá»‡u tiÃªu thá»¥',
            'Sá»‘ tÃºi khÃ­': 'Sá»‘ tÃºi khÃ­',
            'Sá»‘ cá»­a': 'Sá»‘ cá»­a',
            'HÃ£ng': 'HÃ£ng xe',
            'DÃ²ng xe': 'DÃ²ng xe',
            'NÄƒm sáº£n xuáº¥t': 'NÄƒm sáº£n xuáº¥t',
            'PhiÃªn báº£n xe': 'PhiÃªn báº£n xe',
            'Há»™p sá»‘': 'Há»™p sá»‘',
            'NhiÃªn liá»‡u': 'NhiÃªn liá»‡u',
            'Kiá»ƒu dÃ¡ng': 'Kiá»ƒu dÃ¡ng',
            'Sá»‘ chá»—': 'Sá»‘ chá»—',
            'Khoáº£ng sÃ¡ng gáº§m xe': 'Khoáº£ng sÃ¡ng gáº§m xe',
            'Trá»ng lÆ°á»£ng': 'Trá»ng lÆ°á»£ng',
            'Trá»ng táº£i': 'Trá»ng táº£i'
        }
        
        for label_text, column_name in label_map.items():
            if column_name in specs:
                continue
                
            # TÃ¬m span chá»©a label text
            label_spans = soup.find_all('span', class_='bwq0cbs', 
                                       string=lambda t: t and label_text in t)
            
            for label_span in label_spans:
                # TÃ¬m span káº¿ tiáº¿p cÃ¹ng level (sibling)
                next_span = label_span.find_next_sibling('span', class_='bwq0cbs')
                
                if next_span:
                    value = next_span.get_text(strip=True)
                    if value:
                        specs[column_name] = value
                        print(f"       âœ“ {column_name}: {value}")
                        break
                
                # Náº¿u khÃ´ng cÃ³ sibling, tÃ¬m trong parent
                parent = label_span.parent
                if parent:
                    all_spans = parent.find_all('span', class_='bwq0cbs')
                    # Náº¿u cÃ³ 2 span, láº¥y span thá»© 2
                    if len(all_spans) >= 2:
                        value = all_spans[1].get_text(strip=True)
                        if value and value != label_text:
                            specs[column_name] = value
                            print(f"       âœ“ {column_name}: {value}")
                            break
        
        if len(specs) > 0:
            print(f"    âœ… TÃ¬m thÃªm {len(specs)} thÃ´ng sá»‘ qua label matching")
        
        return specs
    
    def extract_specs(self, soup):
        """
        Káº¿t há»£p 2 phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ Ä‘áº£m báº£o 100% láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u
        """
        specs = {}
        
        try:
            print(f"    ğŸ” Báº®T Äáº¦U CÃ€O THÃ”NG Sá»...")
            
            # PhÆ°Æ¡ng phÃ¡p 1: itemprop (chÃ­nh xÃ¡c nháº¥t)
            specs1 = self.extract_specs_by_itemprop(soup)
            specs.update(specs1)
            
            # PhÆ°Æ¡ng phÃ¡p 2: Label matching (backup)
            specs2 = self.extract_specs_by_label_matching(soup)
            # Chá»‰ thÃªm nhá»¯ng trÆ°á»ng chÆ°a cÃ³
            for key, value in specs2.items():
                if key not in specs:
                    specs[key] = value
            
            print(f"    ğŸ¯ Tá»”NG: {len(specs)} thÃ´ng sá»‘")
            
            # Debug: Kiá»ƒm tra cÃ¡c trÆ°á»ng quan trá»ng
            critical_fields = ['Há»‡ dáº«n Ä‘á»™ng', 'CÃ´ng suáº¥t Ä‘á»™ng cÆ¡', 'Momen xoáº¯n', 
                             'Dung tÃ­ch Ä‘á»™ng cÆ¡', 'NhiÃªn liá»‡u tiÃªu thá»¥', 'Sá»‘ tÃºi khÃ­', 'Sá»‘ cá»­a']
            
            missing = [f for f in critical_fields if f not in specs]
            if missing:
                print(f"    âš ï¸  CÃ²n thiáº¿u: {', '.join(missing)}")
            else:
                print(f"    âœ… Äáº¦Y Äá»¦ táº¥t cáº£ cÃ¡c trÆ°á»ng quan trá»ng!")
            
        except Exception as e:
            print(f"    âš  Lá»—i: {str(e)}")
            import traceback
            traceback.print_exc()
        
        return specs
    
    def scrape_product(self, url):
        """CÃ o thÃ´ng tin chi tiáº¿t"""
        try:
            self.driver.get(url)
            time.sleep(3)  # TÄƒng thá»i gian chá»
            
            # Scroll nhiá»u hÆ¡n Ä‘á»ƒ load Ä‘áº§y Ä‘á»§
            for i in range(3):
                self.driver.execute_script(f"window.scrollTo(0, {(i+1)*800});")
                time.sleep(1)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            product_data = {'URL': url, 'TiÃªu Ä‘á»': '', 'GiÃ¡': ''}
            
            # Láº¥y tiÃªu Ä‘á»
            title_elem = soup.find('h1')
            if title_elem:
                product_data['TiÃªu Ä‘á»'] = title_elem.get_text(strip=True)
            
            # Láº¥y giÃ¡
            price_elem = soup.find('b', class_='p26z2wb')
            if price_elem:
                product_data['GiÃ¡'] = price_elem.get_text(strip=True)
            else:
                price_patterns = [
                    soup.find(string=re.compile(r'\d+\.\d+\.\d+ Ä‘')),
                    soup.find(string=re.compile(r'\d+ triá»‡u')),
                    soup.find(string=re.compile(r'\d+\.\d+ tá»·')),
                ]
                for p in price_patterns:
                    if p:
                        product_data['GiÃ¡'] = p.strip()
                        break
            
            # Láº¥y thÃ´ng sá»‘
            specs = self.extract_specs(soup)
            product_data.update(specs)
            
            return product_data
            
        except Exception as e:
            print(f"    âœ— Lá»—i: {str(e)}")
            return None
    
    def go_to_next_page_direct(self, next_page):
        """Chuyá»ƒn trang"""
        try:
            new_url = f"https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh?page={next_page}"
            print(f"\nâ¡ï¸  Chuyá»ƒn sang trang {next_page}...")
            self.driver.get(new_url)
            time.sleep(4)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            products = soup.find_all('a', href=re.compile(r'/mua-ban.*\d+'))
            
            return len(products) > 0
                
        except Exception as e:
            print(f"\nâœ— Lá»—i: {str(e)}")
            return False
    
    def scrape_test_pages(self, start_url, num_pages=2):
        """CÃ€O TEST"""
        print("=" * 70)
        print("ğŸ”§ FIX 100% - Æ¯U TIÃŠN ITEMPROP")
        print("=" * 70)
        print(f"ğŸ”— URL: {start_url}\n")
        
        self.driver.get(start_url)
        time.sleep(3)
        print(f"âœ“ ÄÃ£ vÃ o: {self.driver.title}\n")
        
        for page_num in range(1, num_pages + 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“„ TRANG {page_num}/{num_pages}")
            print(f"{'='*70}")
            
            if page_num > 1:
                if not self.go_to_next_page_direct(page_num):
                    break
            
            product_links = self.get_product_links_from_page()
            
            if not product_links:
                print(f"âš  Trang trá»‘ng!")
                break
            
            print(f"\nğŸ”„ CÃ o {len(product_links)} sáº£n pháº©m...\n")
            
            for i, link in enumerate(product_links, 1):
                print(f"{'='*50}")
                print(f"  [{page_num}.{i}/{len(product_links)}]")
                print(f"{'='*50}")
                
                product_data = self.scrape_product(link)
                
                if product_data and product_data.get('TiÃªu Ä‘á»'):
                    self.data.append(product_data)
                    print(f"  âœ… {product_data.get('TiÃªu Ä‘á»', '')[:50]}")
                    print(f"     ğŸ’° {product_data.get('GiÃ¡', 'N/A')}")
                    
                    spec_count = len([k for k in product_data.keys() if k not in ['URL', 'TiÃªu Ä‘á»', 'GiÃ¡']])
                    print(f"     ğŸ“Š {spec_count} thÃ´ng sá»‘")
                else:
                    print(f"  âœ— Lá»—i")
                
                time.sleep(1)
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ HOÃ€N Táº¤T: {len(self.data)} sáº£n pháº©m")
        print(f"{'='*70}")
    
    def export_to_excel(self, filename='chotot_100percent.xlsx'):
        """Xuáº¥t Excel vá»›i thá»‘ng kÃª chi tiáº¿t"""
        if not self.data:
            print("\nâœ— KhÃ´ng cÃ³ dá»¯ liá»‡u!")
            return
        
        df = pd.DataFrame(self.data)
        
        for col in self.required_columns:
            if col not in df.columns:
                df[col] = ''
        
        df = df[self.required_columns]
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"\n{'='*70}")
        print(f"ğŸ’¾ ÄÃƒ LÆ¯U: {filename}")
        print(f"ğŸ“Š {len(self.data)} sáº£n pháº©m Ã— {len(self.required_columns)} cá»™t")
        print(f"{'='*70}")
        
        print(f"\nğŸ“Š THá»NG KÃŠ:")
        
        print(f"\n   ğŸ”§ CÃC TRÆ¯á»œNG ÄÃƒ FIX:")
        critical = ['Há»‡ dáº«n Ä‘á»™ng', 'CÃ´ng suáº¥t Ä‘á»™ng cÆ¡', 'Momen xoáº¯n', 
                   'Dung tÃ­ch Ä‘á»™ng cÆ¡', 'NhiÃªn liá»‡u tiÃªu thá»¥', 'Sá»‘ tÃºi khÃ­', 'Sá»‘ cá»­a']
        
        for col in critical:
            if col in df.columns:
                non_empty = df[col].astype(str).str.strip().ne('').sum()
                pct = non_empty*100//len(df) if len(df) > 0 else 0
                
                if pct >= 80:
                    status = "âœ…"
                elif pct >= 50:
                    status = "âš ï¸"
                else:
                    status = "âŒ"
                
                print(f"      {status} {col}: {non_empty}/{len(df)} ({pct}%)")
        
        print(f"\n   âš™ï¸  THÃ”NG Sá» KHÃC:")
        others = ['HÃ£ng xe', 'DÃ²ng xe', 'NÄƒm sáº£n xuáº¥t', 'Há»™p sá»‘', 'NhiÃªn liá»‡u']
        
        for col in others:
            if col in df.columns:
                non_empty = df[col].astype(str).str.strip().ne('').sum()
                pct = non_empty*100//len(df) if len(df) > 0 else 0
                status = "âœ…" if pct > 50 else "âš ï¸"
                print(f"      {status} {col}: {non_empty}/{len(df)} ({pct}%)")
        
        print(f"\n{'='*70}")
        
        # Kiá»ƒm tra cÃ¡c trÆ°á»ng quan trá»ng
        all_good = True
        for col in critical:
            if col in df.columns:
                non_empty = df[col].astype(str).str.strip().ne('').sum()
                if non_empty == 0:
                    all_good = False
                    print(f"âš ï¸  Cáº¢NH BÃO: {col} váº«n cÃ²n TRá»NG!")
        
        if all_good:
            print("âœ… HOÃ€N Háº¢O - Táº¥t cáº£ cÃ¡c trÆ°á»ng Ä‘á»u cÃ³ dá»¯ liá»‡u!")
        
        print(f"{'='*70}")
    
    def close(self):
        """ÄÃ³ng browser"""
        self.driver.quit()


def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ Chá»£ Tá»‘t Scraper - FIX 100%")
    print("=" * 70)
    print("ğŸ”‘ Chiáº¿n lÆ°á»£c:")
    print("   1. Æ¯u tiÃªn itemprop (chÃ­nh xÃ¡c 100%)")
    print("   2. Backup báº±ng label matching")
    print("   3. TÄƒng thá»i gian chá» Ä‘á»ƒ load Ä‘áº§y Ä‘á»§")
    print("=" * 70)
    print()
    
    url = "https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh"
    scraper = ChototScraper(headless=True)
    
    try:
        scraper.scrape_test_pages(url, num_pages=2)
        scraper.export_to_excel('chotot_100percent.xlsx')
        
    except KeyboardInterrupt:
        print("\n\nâš  Dá»«ng")
        if scraper.data:
            scraper.export_to_excel('chotot_interrupted.xlsx')
    except Exception as e:
        print(f"\nâœ— Lá»—i: {str(e)}")
        import traceback
        traceback.print_exc()
        if scraper.data:
            scraper.export_to_excel('chotot_error.xlsx')
    finally:
        print("\nğŸ”’ ÄÃ³ng browser...")
        scraper.close()
        print("âœ… HoÃ n táº¥t!")


if __name__ == "__main__":
    main()