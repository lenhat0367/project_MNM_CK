"""
Web Scraper cho Ch·ª£ T·ªët - C√†o d·ªØ li·ªáu xe m√°y (FIXED)
S·ª≠a l·ªói: Chuy·ªÉn trang v√† tr√≠ch xu·∫•t th√¥ng s·ªë k·ªπ thu·∫≠t
"""

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class ChototScraper:
    def __init__(self, headless=True):
        """Kh·ªüi t·∫°o scraper"""
        gecko_path = "/Users/binh/thuc_hanh_ma_nguon_mo/gecko b√†i t·∫≠p /b√†i t·∫≠p tr√™n l·ªõp/geckodriver"
        firefox_path = "/Applications/Firefox.app/Contents/MacOS/firefox"
        
        self.options = Options()
        self.options.binary_location = firefox_path
        self.options.page_load_strategy = 'eager'
        
        if headless:
            self.options.add_argument('--headless')
        
        self.options.set_preference('permissions.default.image', 2)
        self.options.set_preference('general.useragent.override', 
                                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/120.0')
        
        service = Service(executable_path=gecko_path)
        self.driver = webdriver.Firefox(service=service, options=self.options)
        self.driver.set_page_load_timeout(20)
        self.wait = WebDriverWait(self.driver, 15)
        self.data = []
    
    def step1_go_to_homepage(self):
        """B∆∞·ªõc 1: V√†o trang ch·ªß Ch·ª£ T·ªët v√† ch·ªçn khu v·ª±c"""
        print("\n[B∆Ø·ªöC 1] Truy c·∫≠p trang ch·ªß https://www.chotot.com/")
        self.driver.get("https://www.chotot.com/")
        time.sleep(3)
        print(f"‚úì ƒê√£ v√†o trang: {self.driver.title}")
        
        print("\n[B∆Ø·ªöC 1.1] X·ª≠ l√Ω popup ch·ªçn khu v·ª±c...")
        try:
            hcm_selectors = [
                (By.XPATH, "//*[contains(text(), 'H·ªì Ch√≠ Minh')]"),
                (By.XPATH, "//img[@alt='H·ªì Ch√≠ Minh']/ancestor::*[1]"),
                (By.XPATH, "//*[text()='H·ªì Ch√≠ Minh']"),
            ]
            
            clicked = False
            for by, selector in hcm_selectors:
                try:
                    hcm_element = self.wait.until(EC.element_to_be_clickable((by, selector)))
                    hcm_element.click()
                    print(f"‚úì ƒê√£ ch·ªçn 'H·ªì Ch√≠ Minh'")
                    clicked = True
                    time.sleep(2)
                    break
                except:
                    continue
            
            if not clicked:
                print("‚Ñπ Kh√¥ng t√¨m th·∫•y popup khu v·ª±c, c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c ch·ªçn s·∫µn")
            
            try:
                confirm_selectors = [
                    (By.XPATH, "//button[contains(text(), 'X√°c nh·∫≠n')]"),
                    (By.XPATH, "//*[contains(text(), 'X√°c nh·∫≠n')]"),
                    (By.XPATH, "//button[contains(@class, 'confirm') or contains(@class, 'submit')]"),
                ]
                
                for by, selector in confirm_selectors:
                    try:
                        confirm_btn = self.wait.until(EC.element_to_be_clickable((by, selector)))
                        confirm_btn.click()
                        print(f"‚úì ƒê√£ click 'X√°c nh·∫≠n'")
                        time.sleep(3)
                        break
                    except:
                        continue
            except:
                print("‚Ñπ Kh√¥ng t√¨m th·∫•y n√∫t X√°c nh·∫≠n")
                
        except Exception as e:
            print(f"‚Ñπ B·ªè qua popup: {str(e)}")
        
        print(f"‚úì URL sau khi ch·ªçn khu v·ª±c: {self.driver.current_url}")
    
    def step2_click_xe_co(self):
        """B∆∞·ªõc 2: Click v√†o category 'Xe c·ªô'"""
        print("\n[B∆Ø·ªöC 2] Click v√†o 'Xe c·ªô'")
        time.sleep(2)
        
        try:
            selectors = [
                (By.XPATH, "//*[text()='Xe c·ªô']"),
                (By.XPATH, "//*[contains(text(), 'Xe c·ªô')]"),
                (By.XPATH, "//img[@alt='Xe c·ªô']"),
                (By.XPATH, "//img[contains(@alt, 'Xe c·ªô')]/ancestor::a"),
                (By.XPATH, "//img[contains(@alt, 'Xe c·ªô')]/parent::*/parent::*"),
                (By.XPATH, "//span[@class='coblrut' and text()='Xe c·ªô']"),
                (By.XPATH, "//span[contains(@class, 'coblrut') and contains(text(), 'Xe c·ªô')]"),
                (By.XPATH, "//a[contains(., 'Xe c·ªô')]"),
            ]
            
            for by, selector in selectors:
                try:
                    print(f"   Th·ª≠ selector: {selector[:50]}...")
                    element = self.wait.until(EC.presence_of_element_located((by, selector)))
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", element)
                    time.sleep(1)
                    element.click()
                    print(f"‚úì ƒê√£ click v√†o 'Xe c·ªô'")
                    time.sleep(3)
                    print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                    return True
                except Exception as e:
                    continue
            
            with open('debug_homepage.html', 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            print("‚úó Kh√¥ng t√¨m th·∫•y 'Xe c·ªô', ƒë√£ l∆∞u debug_homepage.html")
            return False
            
        except Exception as e:
            print(f"‚úó L·ªói: {str(e)}")
            return False
    
    def step3_click_xem_them(self):
        """B∆∞·ªõc 3: Click n√∫t 'Xem th√™m X tin ƒëƒÉng'"""
        print("\n[B∆Ø·ªöC 3] T√¨m v√† click 'Xem th√™m ... tin ƒëƒÉng'")
        self.driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        
        try:
            selectors = [
                (By.XPATH, "//*[contains(text(), 'Xem th√™m') and contains(text(), 'tin ƒëƒÉng')]"),
                (By.XPATH, "//button[contains(text(), 'Xem th√™m')]"),
                (By.XPATH, "//a[contains(text(), 'Xem th√™m')]"),
                (By.XPATH, "//*[contains(text(), 'tin ƒëƒÉng')]/ancestor::button"),
                (By.XPATH, "//*[contains(text(), 'tin ƒëƒÉng')]/ancestor::a"),
            ]
            
            for by, selector in selectors:
                try:
                    element = self.wait.until(EC.element_to_be_clickable((by, selector)))
                    button_text = element.text
                    print(f"‚úì T√¨m th·∫•y n√∫t: '{button_text}'")
                    element.click()
                    time.sleep(3)
                    print(f"‚úì ƒê√£ click, URL hi·ªán t·∫°i: {self.driver.current_url}")
                    return True
                except:
                    continue
            
            print("‚Ñπ Kh√¥ng t√¨m th·∫•y n√∫t 'Xem th√™m', c√≥ th·ªÉ ƒë√£ ·ªü trang danh s√°ch r·ªìi")
            return True
            
        except Exception as e:
            print(f"‚Ñπ Kh√¥ng click ƒë∆∞·ª£c 'Xem th√™m': {str(e)}")
            return True
    
    def get_product_links_from_page(self):
        """L·∫•y t·∫•t c·∫£ links s·∫£n ph·∫©m t·ª´ trang hi·ªán t·∫°i"""
        product_links = []
        print(f"üì• ƒêang scroll ƒë·ªÉ load s·∫£n ph·∫©m...")
        
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scrolls = 10
        
        while scroll_attempts < max_scrolls:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts += 1
            if new_height == last_height:
                break
            last_height = new_height
        
        print(f"‚úì ƒê√£ scroll {scroll_attempts} l·∫ßn")
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            if href:
                if href.startswith('/'):
                    full_url = 'https://www.chotot.com' + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                if ('chotot.com' in full_url and 
                    re.search(r'/\d+', full_url) and
                    '/mua-ban' in full_url and
                    '?' not in full_url and
                    full_url not in product_links):
                    product_links.append(full_url)
        
        if len(product_links) > 0:
            print(f"‚úì T√¨m th·∫•y {len(product_links)} s·∫£n ph·∫©m")
            print(f"üìù V√≠ d·ª• 3 links ƒë·∫ßu:")
            for i, link in enumerate(product_links[:3], 1):
                print(f"   {i}. {link}")
        
        return product_links
    
    def extract_specs(self, soup):
        """
        Tr√≠ch xu·∫•t th√¥ng s·ªë k·ªπ thu·∫≠t - FIXED VERSION
        X·ª≠ l√Ω c·∫£ 2 lo·∫°i c·∫•u tr√∫c HTML kh√°c nhau
        """
        specs = {}
        
        try:
            # T√¨m h2 "Th√¥ng s·ªë k·ªπ thu·∫≠t" ho·∫∑c "Th√¥ng s·ªë chi ti·∫øt"
            detail_section = soup.find('h2', class_='tfvqu6u', string=re.compile(r'Th√¥ng s·ªë'))
            
            if detail_section:
                # T√¨m container ch√≠nh (div.pqop88r)
                main_container = detail_section.find_next('div', class_='pqop88r')
                
                if main_container:
                    # T√¨m t·∫•t c·∫£ div ch·ª©a th√¥ng s·ªë (c√≥ th·ªÉ l√† s1r2e0fc ho·∫∑c befjs93)
                    spec_containers = main_container.find_all('div', class_=re.compile(r's1r2e0fc|befjs93'))
                    
                    for container in spec_containers:
                        # T√¨m t·∫•t c·∫£ c√°c item th√¥ng s·ªë (pqp26ip ho·∫∑c p1ja3eq0)
                        spec_items = container.find_all('div', class_=re.compile(r'pqp26ip|p1ja3eq0'))
                        
                        for item in spec_items:
                            # T√¨m t·∫•t c·∫£ span
                            all_spans = item.find_all('span', class_='bwq0cbs')
                            
                            if len(all_spans) >= 2:
                                # Span ƒë·∫ßu ti√™n c√≥ m√†u x√°m (rgb(140, 140, 140)) = label
                                # Span th·ª© 2 = value
                                label_span = all_spans[0]
                                value_span = all_spans[1]
                                
                                # L·∫•y text v√† l√†m s·∫°ch
                                label = label_span.get_text(strip=True).replace(':', '').strip()
                                value = value_span.get_text(strip=True)
                                
                                if label and value:
                                    specs[label] = value
                            
                            # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: c√≥ th·ªÉ c√≥ link <a> trong value
                            # V√≠ d·ª•: H√£ng xe, D√≤ng xe
                            elif len(all_spans) == 1:
                                label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                # T√¨m link ho·∫∑c span ti·∫øp theo
                                link = item.find('a')
                                if link:
                                    value_span = link.find('span', class_='bwq0cbs')
                                    if value_span:
                                        value = value_span.get_text(strip=True)
                                        if label and value:
                                            specs[label] = value
        
        except Exception as e:
            print(f"    ‚ö† L·ªói tr√≠ch xu·∫•t specs: {str(e)}")
        
        return specs
    
    def scrape_product(self, url):
        """C√†o th√¥ng tin chi ti·∫øt m·ªôt s·∫£n ph·∫©m"""
        try:
            self.driver.get(url)
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 800);")
            time.sleep(1)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            product_data = {
                'URL': url,
                'Ti√™u ƒë·ªÅ': '',
                'Gi√°': '',
            }
            
            # L·∫•y ti√™u ƒë·ªÅ
            title_elem = soup.find('h1')
            if title_elem:
                product_data['Ti√™u ƒë·ªÅ'] = title_elem.get_text(strip=True)
            
            # L·∫•y gi√°
            price_patterns = [
                soup.find(string=re.compile(r'\d+\.\d+\.\d+ ƒë')),
                soup.find(string=re.compile(r'\d+ tri·ªáu')),
                soup.find(string=re.compile(r'\d+\.\d+ t·ª∑')),
            ]
            for price_elem in price_patterns:
                if price_elem:
                    product_data['Gi√°'] = price_elem.strip()
                    break
            
            # L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t
            specs = self.extract_specs(soup)
            product_data.update(specs)
            
            return product_data
            
        except Exception as e:
            print(f"    ‚úó L·ªói c√†o s·∫£n ph·∫©m: {str(e)}")
            return None
    
    def go_to_next_page(self, current_page):
        """
        Chuy·ªÉn sang trang ti·∫øp theo - FIXED VERSION
        """
        next_page = current_page + 1
        
        try:
            # C√ÅCH 1: Click v√†o link s·ªë trang ti·∫øp theo
            # V√≠ d·ª•: <a href="/mua-ban-xe-tp-ho-chi-minh?page=2"><span>2</span></a>
            next_page_link = self.driver.find_element(
                By.XPATH, 
                f"//a[@href='/mua-ban-xe-tp-ho-chi-minh?page={next_page}']"
            )
            
            print(f"\n‚û°Ô∏è  T√¨m th·∫•y link trang {next_page}, ƒëang click...")
            
            # Scroll ƒë·∫øn element
            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_page_link)
            time.sleep(1)
            
            # Click
            next_page_link.click()
            time.sleep(4)
            
            print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
            print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
            return True
            
        except Exception as e1:
            # C√ÅCH 2: Click v√†o n√∫t m≈©i t√™n ph·∫£i (rightIcon)
            try:
                right_arrow = self.driver.find_element(
                    By.XPATH,
                    "//button[@class='Paging_redirectPageBtn__KvsqJ']//i[contains(@class, 'rightIcon') and not(contains(@class, 'Disable'))]"
                )
                
                # Click v√†o button cha
                button = right_arrow.find_element(By.XPATH, "./ancestor::button")
                
                print(f"\n‚û°Ô∏è  T√¨m th·∫•y n√∫t m≈©i t√™n ph·∫£i, ƒëang click...")
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                time.sleep(1)
                button.click()
                time.sleep(4)
                
                print(f"‚úì ƒê√£ chuy·ªÉn sang trang ti·∫øp theo")
                print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                return True
                
            except Exception as e2:
                # C√ÅCH 3: Thay ƒë·ªïi URL tr·ª±c ti·∫øp
                try:
                    new_url = f"https://www.chotot.com/mua-ban-xe-tp-ho-chi-minh?page={next_page}"
                    print(f"\n‚û°Ô∏è  Kh√¥ng t√¨m th·∫•y n√∫t, ƒëang chuy·ªÉn URL tr·ª±c ti·∫øp...")
                    self.driver.get(new_url)
                    time.sleep(4)
                    
                    print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
                    print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                    return True
                    
                except Exception as e3:
                    print(f"\n‚úó Kh√¥ng th·ªÉ chuy·ªÉn trang: {str(e3)}")
                    return False
    
    def scrape_all_pages(self, max_products=100):
        """C√†o t·∫•t c·∫£ s·∫£n ph·∫©m t·ª´ nhi·ªÅu trang"""
        print("=" * 70)
        print("B·∫ÆT ƒê·∫¶U QUY TR√åNH C√ÄO D·ªÆ LI·ªÜU")
        print("=" * 70)
        
        self.step1_go_to_homepage()
        
        if not self.step2_click_xe_co():
            print("‚úó Kh√¥ng th·ªÉ ti·∫øp t·ª•c, d·ª´ng l·∫°i")
            return
        
        self.step3_click_xem_them()
        
        print("\n" + "=" * 70)
        print("[B∆Ø·ªöC 4] B·∫ÆT ƒê·∫¶U C√ÄO S·∫¢N PH·∫®M")
        print("=" * 70)
        
        page_num = 1
        
        while len(self.data) < max_products:
            print(f"\n{'='*70}")
            print(f"üìÑ TRANG {page_num}")
            print(f"üîó URL: {self.driver.current_url}")
            print(f"{'='*70}")
            
            product_links = self.get_product_links_from_page()
            
            if len(product_links) == 0:
                print("‚ö† Kh√¥ng c√≤n s·∫£n ph·∫©m, d·ª´ng l·∫°i")
                break
            
            remaining = max_products - len(self.data)
            product_links = product_links[:remaining]
            
            print(f"\nüîÑ C√†o {len(product_links)} s·∫£n ph·∫©m t·ª´ trang n√†y...")
            
            for i, link in enumerate(product_links, 1):
                print(f"\n  [{page_num}.{i}/{len(product_links)}] ", end="")
                product_data = self.scrape_product(link)
                
                if product_data and product_data.get('Ti√™u ƒë·ªÅ'):
                    self.data.append(product_data)
                    print(f"‚úì {product_data.get('Ti√™u ƒë·ªÅ', '')[:50]}")
                    print(f"    üí∞ {product_data.get('Gi√°', 'N/A')}")
                    
                    spec_count = len([k for k in product_data.keys() if k not in ['URL', 'Ti√™u ƒë·ªÅ', 'Gi√°']])
                    if spec_count > 0:
                        print(f"    üìä {spec_count} th√¥ng s·ªë")
                else:
                    print(f"‚úó L·ªói")
                
                time.sleep(0.5)
            
            print(f"\n‚úì Ho√†n th√†nh trang {page_num}")
            print(f"üìä T·ªïng ƒë√£ c√†o: {len(self.data)}/{max_products}")
            
            # Ki·ªÉm tra xem c√≤n c√†o ti·∫øp kh√¥ng
            if len(self.data) >= max_products:
                print("\nüéØ ƒê√£ ƒë·ªß s·ªë l∆∞·ª£ng s·∫£n ph·∫©m c·∫ßn c√†o")
                break
            
            # Chuy·ªÉn sang trang ti·∫øp theo
            if not self.go_to_next_page(page_num):
                print("\n‚ö† Kh√¥ng th·ªÉ chuy·ªÉn trang, d·ª´ng l·∫°i")
                break
            
            page_num += 1
        
        print(f"\n{'='*70}")
        print(f"üéâ HO√ÄN T·∫§T: ƒê√£ c√†o {len(self.data)} s·∫£n ph·∫©m")
        print(f"{'='*70}")
    
    def export_to_excel(self, filename='chotot_xemay_data.xlsx'):
        """Xu·∫•t d·ªØ li·ªáu ra Excel"""
        if not self.data:
            print("\n‚úó Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t!")
            return
        
        df = pd.DataFrame(self.data)
        
        # S·∫Øp x·∫øp c·ªôt
        priority_cols = [
            'URL', 'Ti√™u ƒë·ªÅ', 'Gi√°',
            'H√£ng xe', 'D√≤ng xe', 'NƒÉm s·∫£n xu·∫•t', 'H·ªôp s·ªë', 'Nhi√™n li·ªáu', 
            'Ki·ªÉu d√°ng', 'S·ªë ch·ªó', 'Tr·ªçng l∆∞·ª£ng', 'Tr·ªçng t·∫£i',
            'S·ªë Km ƒë√£ ƒëi', 'T√¨nh tr·∫°ng', 'Xu·∫•t x·ª©', 
            'C√≥ ph·ª• ki·ªán ƒëi k√®m', 'C√≤n h·∫°n ƒëƒÉng ki·ªÉm',
            'Ch√≠nh s√°ch b·∫£o h√†nh',
            'Lo·∫°i xe', 'Dung t√≠ch xe', 'Lo·∫°i ph·ª• t√πng', 'M√£ ph·ª• t√πng'
        ]
        
        existing_cols = [col for col in priority_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in priority_cols]
        df = df[existing_cols + other_cols]
        
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"\n{'='*70}")
        print(f"üíæ ƒê√É L∆ØU FILE")
        print(f"üìÅ {filename}")
        print(f"üìä {len(self.data)} s·∫£n ph·∫©m, {len(df.columns)} c·ªôt")
        print(f"{'='*70}")
    
    def close(self):
        """ƒê√≥ng browser"""
        self.driver.quit()


def main():
    """H√†m ch√≠nh"""
    print("üöÄ Kh·ªüi ƒë·ªông Ch·ª£ T·ªët Scraper (FIXED VERSION)...")
    
    scraper = ChototScraper(headless=True)
    
    try:
        scraper.scrape_all_pages(max_products=100)
        scraper.export_to_excel('chotot_xemay_data.xlsx')
        
    except KeyboardInterrupt:
        print("\n\n‚ö† D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        if len(scraper.data) > 0:
            scraper.export_to_excel('chotot_partial.xlsx')
    except Exception as e:
        print(f"\n‚úó L·ªói: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nüîí ƒê√≥ng browser...")
        scraper.close()
        print("‚úÖ Ho√†n t·∫•t!")


if __name__ == "__main__":
    main()