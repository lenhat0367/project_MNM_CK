"""
Web Scraper cho Ch·ª£ T·ªët - Version 8.1 (DATABASE + CSV AUTO-EXPORT)
Gi·ªØ nguy√™n 100% logic c√†o c·ªßa b·∫°n.
"""

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
# --- TH√äM: Th∆∞ vi·ªán MongoDB ---
from pymongo import MongoClient

class ChototScraper:
    def __init__(self, headless=True):
        """Kh·ªüi t·∫°o scraper"""
        gecko_path = r'D:\Cong viec\Ma_Nguon_Mo\BT_cuoiki_MNM\geckodriver.exe'
        firefox_path = "C:/Program Files/Mozilla Firefox/firefox.exe"
        
        self.options = Options()
        self.options.binary_location = firefox_path
        self.options.page_load_strategy = 'eager'
        
        if headless:
            self.options.add_argument('--headless')
        
        self.options.set_preference('general.useragent.override', 
                                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/120.0')
        
        service = Service(executable_path=gecko_path)
        self.driver = webdriver.Firefox(service=service, options=self.options)
        self.driver.set_page_load_timeout(20)
        self.wait = None
        self.data = []

        # --- TH√äM: K·∫øt n·ªëi MongoDB ---
        try:
            self.client = MongoClient("mongodb://localhost:27017/")
            self.db = self.client["chotot_db"]
            self.collection = self.db["xe_may"]
            self.collection.create_index("URL", unique=True) # Ch·ªëng tr√πng link
            print("‚úì ƒê√£ k·∫øt n·ªëi MongoDB th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚úó L·ªói k·∫øt n·ªëi DB: {e}")
        
        # DANH S√ÅCH C·ªòT (Gi·ªØ nguy√™n)
        self.required_columns = [
            'URL', 'T√™n s·∫£n ph·∫©m', 'Gi√°', 'T√™n ng∆∞·ªùi ƒëƒÉng', 'ƒê·ªãa ch·ªâ', 'Th·ªùi gian ƒëƒÉng', 'URL h√¨nh ·∫£nh',
            'S·ªë Km ƒë√£ ƒëi', 'S·ªë ƒë·ªùi ch·ªß', 'C√≥ ph·ª• ki·ªán ƒëi k√®m', 'C√≤n h·∫°n ƒëƒÉng ki·ªÉm',
            'Xu·∫•t x·ª©', 'T√¨nh tr·∫°ng', 'Ch√≠nh s√°ch b·∫£o h√†nh',
            'H√£ng xe', 'D√≤ng xe', 'NƒÉm s·∫£n xu·∫•t', 'H·ªôp s·ªë',
            'Nhi√™n li·ªáu', 'Ki·ªÉu d√°ng', 'S·ªë ch·ªó', 
            'Tr·ªçng l∆∞·ª£ng', 'Tr·ªçng t·∫£i'
        ]
        
        # MAPPING ITEMPROP (Gi·ªØ nguy√™n)
        self.itemprop_mappings = {
            'mileage_v2': 'S·ªë Km ƒë√£ ƒëi', 'number_of_owners': 'S·ªë ƒë·ªùi ch·ªß',
            'include_accessories': 'C√≥ ph·ª• ki·ªán ƒëi k√®m', 'valid_registration': 'C√≤n h·∫°n ƒëƒÉng ki·ªÉm',
            'carorigin': 'Xu·∫•t x·ª©', 'condition_ad': 'T√¨nh tr·∫°ng',
            'veh_warranty_policy': 'Ch√≠nh s√°ch b·∫£o h√†nh', 'carbrand': 'H√£ng xe',
            'carmodel': 'D√≤ng xe', 'mfdate': 'NƒÉm s·∫£n xu·∫•t',
            'gearbox': 'H·ªôp s·ªë', 'fuel': 'Nhi√™n li·ªáu',
            'cartype': 'Ki·ªÉu d√°ng', 'carseats': 'S·ªë ch·ªó',
            'veh_unladen_weight': 'Tr·ªçng l∆∞·ª£ng', 'veh_gross_weight': 'Tr·ªçng t·∫£i'
        }

    # --- GI·ªÆ NGUY√äN 100% C√ÅC H√ÄM C√ÄO D·ªÆ LI·ªÜU C·ª¶A B·∫†N ---
    def get_product_links_from_page(self):
        product_links = []
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        while scroll_attempts < 10:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts += 1
            if new_height == last_height: break
            last_height = new_height
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href')
            if href:
                full_url = 'https://xe.chotot.com' + href if href.startswith('/') else href
                if ('chotot.com' in full_url and re.search(r'/\d+', full_url) and 
                    '/mua-ban' in full_url and '?' not in full_url and full_url not in product_links):
                    product_links.append(full_url)
        return product_links

    def extract_specs_by_itemprop(self, soup):
        specs = {}
        for prop, label in self.itemprop_mappings.items():
            elem = soup.find(itemprop=prop)
            if elem:
                value = elem.get_text(strip=True)
                if value: specs[label] = value
        return specs

    def extract_seller_info(self, soup):
        seller_info = {'T√™n ng∆∞·ªùi ƒëƒÉng': '', 'ƒê·ªãa ch·ªâ': '', 'Th·ªùi gian ƒëƒÉng': '', 'URL h√¨nh ·∫£nh': ''}
        try:
            seller_container = soup.find('div', itemprop='seller')
            if seller_container:
                pf9ruvz_div = seller_container.find('div', class_=re.compile(r'pf9ruvz'))
                if pf9ruvz_div:
                    seller_link = pf9ruvz_div.find('a', href=re.compile(r'/(cua-hang|user)/'))
                    if seller_link:
                        seller_b = seller_link.find('b')
                        if seller_b: seller_info['T√™n ng∆∞·ªùi ƒëƒÉng'] = seller_b.get_text(strip=True)
            time_posted = soup.find('span', class_='bwq0cbs', string=re.compile(r'ƒêƒÉng.*tr∆∞·ªõc'))
            if time_posted: seller_info['Th·ªùi gian ƒëƒÉng'] = time_posted.get_text(strip=True)
            address_spans = soup.find_all('span', class_='bwq0cbs')
            for span in address_spans:
                text = span.get_text(strip=True)
                if len(text) > 15 and any(x in text for x in ['Ph∆∞·ªùng', 'Qu·∫≠n', 'TP', 'Huy·ªán']):
                    if 'ƒêƒÉng' not in text: 
                        seller_info['ƒê·ªãa ch·ªâ'] = text
                        break
            # Logic h√¨nh ·∫£nh (gi·ªØ nguy√™n c·ªßa b·∫°n)
            all_imgs = soup.find_all('img', src=True)
            for img in all_imgs:
                src = img.get('src', '')
                if 'cdn.chotot.com' in src:
                    seller_info['URL h√¨nh ·∫£nh'] = src
                    break
        except: pass
        return seller_info

    def scrape_product(self, url):
        try:
            self.driver.get(url)
            time.sleep(4)
            for i in range(3):
                self.driver.execute_script(f"window.scrollTo(0, {(i+1)*800});")
                time.sleep(1.5)
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            product_data = {'URL': url, 'T√™n s·∫£n ph·∫©m': '', 'Gi√°': ''}
            title_elem = soup.find('h1')
            if title_elem: product_data['T√™n s·∫£n ph·∫©m'] = title_elem.get_text(strip=True)
            price_elem = soup.find('b', class_='p26z2wb')
            if price_elem: product_data['Gi√°'] = price_elem.get_text(strip=True)
            product_data.update(self.extract_seller_info(soup))
            product_data.update(self.extract_specs_by_itemprop(soup))
            return product_data
        except: return None

    def go_to_next_page_direct(self, next_page):
        try:
            new_url = f"https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh?page={next_page}"
            self.driver.get(new_url)
            time.sleep(4)
            return True
        except: return False

    def scrape_test_pages(self, start_url, num_pages=2):
        self.driver.get(start_url)
        time.sleep(3)
        for page_num in range(1, num_pages + 1):
            if page_num > 1: self.go_to_next_page_direct(page_num)
            links = self.get_product_links_from_page()
            for link in links:
                data = self.scrape_product(link)
                if data: self.data.append(data)
                print(f" ‚úì ƒê√£ c√†o: {link[:50]}")

    # --- THAY ƒê·ªîI: Chuy·ªÉn Export Excel sang Database + CSV ---
    def save_to_db_and_export_csv(self):
        if not self.data: return
        
        # 1. L∆∞u v√†o MongoDB (Upsert tr√°nh tr√πng)
        print("\n‚è≥ ƒêang l∆∞u v√†o MongoDB...")
        for item in self.data:
            self.collection.update_one({"URL": item["URL"]}, {"$set": item}, upsert=True)
        
        # 2. T·ª± ƒë·ªông ƒë√≥ng g√≥i ra file CSV t·ª´ Database
        print("üì¶ ƒêang xu·∫•t file CSV...")
        cursor = self.collection.find({})
        df = pd.DataFrame(list(cursor))
        
        if not df.empty:
            if '_id' in df.columns: df.drop('_id', axis=1, inplace=True)
            # encoding='utf-8-sig' ƒë·ªÉ Excel m·ªü kh√¥ng l·ªói ti·∫øng Vi·ªát
            df.to_csv('chotot_oto_database.csv', index=False, encoding='utf-8-sig')
            print(f"‚úÖ ƒê√£ l∆∞u xong DB v√† xu·∫•t file 'chotot_oto_database.csv'!")

    def close(self):
        self.driver.quit()
        if hasattr(self, 'client'): self.client.close()

def main():
    url = "https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh"
    scraper = ChototScraper(headless=True)
    try:
        scraper.scrape_test_pages(url, num_pages=150)
        # Thay h√†m c≈© b·∫±ng h√†m m·ªõi
        scraper.save_to_db_and_export_csv()
    finally:
        scraper.close()

if __name__ == "__main__":
    main()