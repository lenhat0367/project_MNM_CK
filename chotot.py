"""
Web Scraper cho Ch·ª£ T·ªët - C√†o d·ªØ li·ªáu xe m√°y (FINAL VERSION)
S·ª≠a l·ªói: 
1. Chuy·ªÉn trang (∆∞u ti√™n n√∫t m≈©i t√™n ph·∫£i)
2. L·∫•y gi√° t·ª´ class p26z2wb
3. CH·ªà l·∫•y section "Th√¥ng s·ªë k·ªπ thu·∫≠t", b·ªè qua "T√¨nh tr·∫°ng xe"
4. G·ªôp "H√£ng" v√† "H√£ng xe" th√†nh 1 c·ªôt
"""

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class ChototScraper:
    def __init__(self, headless=True):
        """Kh·ªüi t·∫°o scraper"""
        gecko_path = "/Users/binh/thuc_hanh_ma_nguon_mo/gecko b√†i t·∫≠p /b√†i t·∫≠p tr√™n l·ªõp/geckodriver"
        firefox_path = "/Applications/Firefox.app/Contents/MacOS/firefox"
        
        self.options = Options()
        self.options.binary_location = firefox_path
        self.options.page_load_strategy = 'eager'
        
        if headless:
            self.options.add_argument('--headless')
        
        self.options.set_preference('permissions.default.image', 2)
        self.options.set_preference('general.useragent.override', 
                                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/120.0')
        
        service = Service(executable_path=gecko_path)
        self.driver = webdriver.Firefox(service=service, options=self.options)
        self.driver.set_page_load_timeout(20)
        self.wait = WebDriverWait(self.driver, 15)
        self.data = []
    
    def step1_go_to_homepage(self):
        """B∆∞·ªõc 1: V√†o trang ch·ªß Ch·ª£ T·ªët v√† ch·ªçn khu v·ª±c"""
        print("\n[B∆Ø·ªöC 1] Truy c·∫≠p trang ch·ªß https://www.chotot.com/")
        self.driver.get("https://www.chotot.com/")
        time.sleep(3)
        print(f"‚úì ƒê√£ v√†o trang: {self.driver.title}")
        
        print("\n[B∆Ø·ªöC 1.1] X·ª≠ l√Ω popup ch·ªçn khu v·ª±c...")
        try:
            hcm_selectors = [
                (By.XPATH, "//*[contains(text(), 'H·ªì Ch√≠ Minh')]"),
                (By.XPATH, "//img[@alt='H·ªì Ch√≠ Minh']/ancestor::*[1]"),
                (By.XPATH, "//*[text()='H·ªì Ch√≠ Minh']"),
            ]
            
            clicked = False
            for by, selector in hcm_selectors:
                try:
                    hcm_element = self.wait.until(EC.element_to_be_clickable((by, selector)))
                    hcm_element.click()
                    print(f"‚úì ƒê√£ ch·ªçn 'H·ªì Ch√≠ Minh'")
                    clicked = True
                    time.sleep(2)
                    break
                except:
                    continue
            
            if not clicked:
                print("‚Ñπ Kh√¥ng t√¨m th·∫•y popup khu v·ª±c, c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c ch·ªçn s·∫µn")
            
            try:
                confirm_selectors = [
                    (By.XPATH, "//button[contains(text(), 'X√°c nh·∫≠n')]"),
                    (By.XPATH, "//*[contains(text(), 'X√°c nh·∫≠n')]"),
                    (By.XPATH, "//button[contains(@class, 'confirm') or contains(@class, 'submit')]"),
                ]
                
                for by, selector in confirm_selectors:
                    try:
                        confirm_btn = self.wait.until(EC.element_to_be_clickable((by, selector)))
                        confirm_btn.click()
                        print(f"‚úì ƒê√£ click 'X√°c nh·∫≠n'")
                        time.sleep(3)
                        break
                    except:
                        continue
            except:
                print("‚Ñπ Kh√¥ng t√¨m th·∫•y n√∫t X√°c nh·∫≠n")
                
        except Exception as e:
            print(f"‚Ñπ B·ªè qua popup: {str(e)}")
        
        print(f"‚úì URL sau khi ch·ªçn khu v·ª±c: {self.driver.current_url}")
    
    def step2_click_xe_co(self):
        """B∆∞·ªõc 2: Click v√†o category 'Xe c·ªô'"""
        print("\n[B∆Ø·ªöC 2] Click v√†o 'Xe c·ªô'")
        time.sleep(2)
        
        try:
            selectors = [
                (By.XPATH, "//*[text()='Xe c·ªô']"),
                (By.XPATH, "//*[contains(text(), 'Xe c·ªô')]"),
                (By.XPATH, "//img[@alt='Xe c·ªô']"),
                (By.XPATH, "//img[contains(@alt, 'Xe c·ªô')]/ancestor::a"),
                (By.XPATH, "//img[contains(@alt, 'Xe c·ªô')]/parent::*/parent::*"),
                (By.XPATH, "//span[@class='coblrut' and text()='Xe c·ªô']"),
                (By.XPATH, "//span[contains(@class, 'coblrut') and contains(text(), 'Xe c·ªô')]"),
                (By.XPATH, "//a[contains(., 'Xe c·ªô')]"),
            ]
            
            for by, selector in selectors:
                try:
                    print(f"   Th·ª≠ selector: {selector[:50]}...")
                    element = self.wait.until(EC.presence_of_element_located((by, selector)))
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", element)
                    time.sleep(1)
                    element.click()
                    print(f"‚úì ƒê√£ click v√†o 'Xe c·ªô'")
                    time.sleep(3)
                    print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                    return True
                except Exception as e:
                    continue
            
            with open('debug_homepage.html', 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            print("‚úó Kh√¥ng t√¨m th·∫•y 'Xe c·ªô', ƒë√£ l∆∞u debug_homepage.html")
            return False
            
        except Exception as e:
            print(f"‚úó L·ªói: {str(e)}")
            return False
    
    def step3_click_xem_them(self):
        """B∆∞·ªõc 3: Click n√∫t 'Xem th√™m X tin ƒëƒÉng'"""
        print("\n[B∆Ø·ªöC 3] T√¨m v√† click 'Xem th√™m ... tin ƒëƒÉng'")
        self.driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        
        try:
            selectors = [
                (By.XPATH, "//*[contains(text(), 'Xem th√™m') and contains(text(), 'tin ƒëƒÉng')]"),
                (By.XPATH, "//button[contains(text(), 'Xem th√™m')]"),
                (By.XPATH, "//a[contains(text(), 'Xem th√™m')]"),
                (By.XPATH, "//*[contains(text(), 'tin ƒëƒÉng')]/ancestor::button"),
                (By.XPATH, "//*[contains(text(), 'tin ƒëƒÉng')]/ancestor::a"),
            ]
            
            for by, selector in selectors:
                try:
                    element = self.wait.until(EC.element_to_be_clickable((by, selector)))
                    button_text = element.text
                    print(f"‚úì T√¨m th·∫•y n√∫t: '{button_text}'")
                    element.click()
                    time.sleep(3)
                    print(f"‚úì ƒê√£ click, URL hi·ªán t·∫°i: {self.driver.current_url}")
                    return True
                except:
                    continue
            
            print("‚Ñπ Kh√¥ng t√¨m th·∫•y n√∫t 'Xem th√™m', c√≥ th·ªÉ ƒë√£ ·ªü trang danh s√°ch r·ªìi")
            return True
            
        except Exception as e:
            print(f"‚Ñπ Kh√¥ng click ƒë∆∞·ª£c 'Xem th√™m': {str(e)}")
            return True
    
    def get_product_links_from_page(self):
        """L·∫•y t·∫•t c·∫£ links s·∫£n ph·∫©m t·ª´ trang hi·ªán t·∫°i"""
        product_links = []
        print(f"üì• ƒêang scroll ƒë·ªÉ load s·∫£n ph·∫©m...")
        
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scrolls = 10
        
        while scroll_attempts < max_scrolls:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts += 1
            if new_height == last_height:
                break
            last_height = new_height
        
        print(f"‚úì ƒê√£ scroll {scroll_attempts} l·∫ßn")
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            if href:
                if href.startswith('/'):
                    full_url = 'https://www.chotot.com' + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                if ('chotot.com' in full_url and 
                    re.search(r'/\d+', full_url) and
                    '/mua-ban' in full_url and
                    '?' not in full_url and
                    full_url not in product_links):
                    product_links.append(full_url)
        
        if len(product_links) > 0:
            print(f"‚úì T√¨m th·∫•y {len(product_links)} s·∫£n ph·∫©m")
            print(f"üìù V√≠ d·ª• 3 links ƒë·∫ßu:")
            for i, link in enumerate(product_links[:3], 1):
                print(f"   {i}. {link}")
        
        return product_links
    
    def extract_specs(self, soup):
        """
        Tr√≠ch xu·∫•t CH·ªà ph·∫ßn "Th√¥ng s·ªë k·ªπ thu·∫≠t"
        B·ªé QUA "T√¨nh tr·∫°ng xe" v√† c√°c section kh√°c
        """
        specs = {}
        
        try:
            detail_section = soup.find('h2', class_='tfvqu6u', string=re.compile(r'Th√¥ng s·ªë'))
            
            if detail_section:
                main_container = detail_section.find_next('div', class_='pqop88r')
                
                if main_container:
                    all_sections = main_container.find_all('div', class_='befjs93')
                    
                    if all_sections:
                        for section in all_sections:
                            section_title = section.find('h3')
                            if section_title and 'Th√¥ng s·ªë k·ªπ thu·∫≠t' in section_title.get_text():
                                spec_items = section.find_all('div', class_=re.compile(r'p1ja3eq0'))
                                
                                for item in spec_items:
                                    all_spans = item.find_all('span', class_='bwq0cbs')
                                    
                                    if len(all_spans) >= 2:
                                        label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                        value = all_spans[1].get_text(strip=True)
                                        
                                        if label and value:
                                            specs[label] = value
                                    
                                    elif len(all_spans) == 1:
                                        label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                        link = item.find('a')
                                        if link:
                                            value_span = link.find('span', class_='bwq0cbs')
                                            if value_span:
                                                value = value_span.get_text(strip=True)
                                                if label and value:
                                                    specs[label] = value
                                
                                break
                    
                    if not specs:
                        spec_container = main_container.find('div', class_='s1r2e0fc')
                        if spec_container:
                            spec_items = spec_container.find_all('div', class_=re.compile(r'pqp26ip|p1ja3eq0'))
                            
                            for item in spec_items:
                                all_spans = item.find_all('span', class_='bwq0cbs')
                                
                                if len(all_spans) >= 2:
                                    label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                    value = all_spans[1].get_text(strip=True)
                                    
                                    if label and value:
                                        specs[label] = value
                                
                                elif len(all_spans) == 1:
                                    label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                    link = item.find('a')
                                    if link:
                                        value_span = link.find('span', class_='bwq0cbs')
                                        if value_span:
                                            value = value_span.get_text(strip=True)
                                            if label and value:
                                                specs[label] = value
        
        except Exception as e:
            print(f"    ‚ö† L·ªói tr√≠ch xu·∫•t specs: {str(e)}")
        
        return specs
    
    def scrape_product(self, url):
        """C√†o th√¥ng tin chi ti·∫øt m·ªôt s·∫£n ph·∫©m"""
        try:
            self.driver.get(url)
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 800);")
            time.sleep(1)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            product_data = {
                'URL': url,
                'Ti√™u ƒë·ªÅ': '',
                'Gi√°': '',
            }
            
            title_elem = soup.find('h1')
            if title_elem:
                product_data['Ti√™u ƒë·ªÅ'] = title_elem.get_text(strip=True)
            
            price_elem = soup.find('b', class_='p26z2wb')
            if price_elem:
                product_data['Gi√°'] = price_elem.get_text(strip=True)
            else:
                price_patterns = [
                    soup.find(string=re.compile(r'\d+\.\d+\.\d+ ƒë')),
                    soup.find(string=re.compile(r'\d+ tri·ªáu')),
                    soup.find(string=re.compile(r'\d+\.\d+ t·ª∑')),
                ]
                for price_elem in price_patterns:
                    if price_elem:
                        product_data['Gi√°'] = price_elem.strip()
                        break
            
            specs = self.extract_specs(soup)
            product_data.update(specs)
            
            if 'H√£ng' in product_data and 'H√£ng xe' not in product_data:
                product_data['H√£ng xe'] = product_data.pop('H√£ng')
            elif 'H√£ng' in product_data and 'H√£ng xe' in product_data:
                product_data.pop('H√£ng')
            
            return product_data
            
        except Exception as e:
            print(f"    ‚úó L·ªói c√†o s·∫£n ph·∫©m: {str(e)}")
            return None
    
    def go_to_next_page(self, current_page):
        """Chuy·ªÉn sang trang ti·∫øp theo - ∆∞u ti√™n n√∫t m≈©i t√™n ph·∫£i"""
        next_page = current_page + 1
        
        try:
            right_arrow_button = self.driver.find_element(
                By.XPATH,
                "//button[@class='Paging_redirectPageBtn__KvsqJ' and .//i[contains(@class, 'rightIcon') and not(contains(@class, 'Disable'))]]"
            )
            
            print(f"\n‚û°Ô∏è  T√¨m th·∫•y n√∫t m≈©i t√™n ph·∫£i, ƒëang click...")
            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", right_arrow_button)
            time.sleep(1)
            right_arrow_button.click()
            time.sleep(4)
            
            print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
            print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
            return True
            
        except Exception as e1:
            print(f"   ‚ö† Kh√¥ng t√¨m th·∫•y n√∫t m≈©i t√™n ph·∫£i")
            
            try:
                next_page_link = self.driver.find_element(
                    By.XPATH, 
                    f"//a[@href='/mua-ban-xe-tp-ho-chi-minh?page={next_page}']"
                )
                
                print(f"\n‚û°Ô∏è  T√¨m th·∫•y link trang {next_page}, ƒëang click...")
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_page_link)
                time.sleep(1)
                next_page_link.click()
                time.sleep(4)
                
                print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
                print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                return True
                
            except Exception as e2:
                print(f"   ‚ö† Kh√¥ng t√¨m th·∫•y link trang {next_page}")
                
                try:
                    new_url = f"https://www.chotot.com/mua-ban-xe-tp-ho-chi-minh?page={next_page}"
                    print(f"\n‚û°Ô∏è  Thay ƒë·ªïi URL tr·ª±c ti·∫øp sang trang {next_page}...")
                    self.driver.get(new_url)
                    time.sleep(4)
                    
                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    product_pattern = re.compile(r'/mua-ban.*\d+$')
                    products = soup.find_all('a', href=product_pattern)
                    
                    if len(products) > 0:
                        print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
                        print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                        return True
                    else:
                        print(f"‚úó Trang {next_page} kh√¥ng c√≥ s·∫£n ph·∫©m, ƒë√£ h·∫øt d·ªØ li·ªáu")
                        return False
                    
                except Exception as e3:
                    print(f"\n‚úó Kh√¥ng th·ªÉ chuy·ªÉn trang: {str(e3)}")
                    return False
    
    def scrape_all_pages(self, max_products=100):
        """C√†o t·∫•t c·∫£ s·∫£n ph·∫©m t·ª´ nhi·ªÅu trang"""
        print("=" * 70)
        print("B·∫ÆT ƒê·∫¶U QUY TR√åNH C√ÄO D·ªÆ LI·ªÜU")
        print("=" * 70)
        
        self.step1_go_to_homepage()
        
        if not self.step2_click_xe_co():
            print("‚úó Kh√¥ng th·ªÉ ti·∫øp t·ª•c, d·ª´ng l·∫°i")
            return
        
        self.step3_click_xem_them()
        
        print("\n" + "=" * 70)
        print("[B∆Ø·ªöC 4] B·∫ÆT ƒê·∫¶U C√ÄO S·∫¢N PH·∫®M")
        print("=" * 70)
        
        page_num = 1
        
        while len(self.data) < max_products:
            print(f"\n{'='*70}")
            print(f"üìÑ TRANG {page_num}")
            print(f"üîó URL: {self.driver.current_url}")
            print(f"{'='*70}")
            
            product_links = self.get_product_links_from_page()
            
            if len(product_links) == 0:
                print("‚ö† Kh√¥ng c√≤n s·∫£n ph·∫©m, d·ª´ng l·∫°i")
                break
            
            remaining = max_products - len(self.data)
            product_links = product_links[:remaining]
            
            print(f"\nüîÑ C√†o {len(product_links)} s·∫£n ph·∫©m t·ª´ trang n√†y...")
            
            for i, link in enumerate(product_links, 1):
                print(f"\n  [{page_num}.{i}/{len(product_links)}] ", end="")
                product_data = self.scrape_product(link)
                
                if product_data and product_data.get('Ti√™u ƒë·ªÅ'):
                    self.data.append(product_data)
                    print(f"‚úì {product_data.get('Ti√™u ƒë·ªÅ', '')[:50]}")
                    print(f"    üí∞ {product_data.get('Gi√°', 'N/A')}")
                    
                    spec_count = len([k for k in product_data.keys() if k not in ['URL', 'Ti√™u ƒë·ªÅ', 'Gi√°']])
                    if spec_count > 0:
                        print(f"    üìä {spec_count} th√¥ng s·ªë")
                else:
                    print(f"‚úó L·ªói")
                
                time.sleep(0.5)
            
            print(f"\n‚úì Ho√†n th√†nh trang {page_num}")
            print(f"üìä T·ªïng ƒë√£ c√†o: {len(self.data)}/{max_products}")
            
            if len(self.data) >= max_products:
                print("\nüéØ ƒê√£ ƒë·ªß s·ªë l∆∞·ª£ng s·∫£n ph·∫©m c·∫ßn c√†o")
                break
            
            if not self.go_to_next_page(page_num):
                print("\n‚ö† Kh√¥ng th·ªÉ chuy·ªÉn trang, d·ª´ng l·∫°i")
                break
            
            page_num += 1
        
        print(f"\n{'='*70}")
        print(f"üéâ HO√ÄN T·∫§T: ƒê√£ c√†o {len(self.data)} s·∫£n ph·∫©m")
        print(f"{'='*70}")
    
    def export_to_excel(self, filename='chotot_xemay_data.xlsx'):
        """Xu·∫•t d·ªØ li·ªáu ra Excel"""
        if not self.data:
            print("\n‚úó Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t!")
            return
        
        df = pd.DataFrame(self.data)
        
        priority_cols = [
            'URL', 'Ti√™u ƒë·ªÅ', 'Gi√°',
            'H√£ng xe', 'D√≤ng xe', 'NƒÉm s·∫£n xu·∫•t', 
            'H·ªôp s·ªë', 'Nhi√™n li·ªáu', 'Ki·ªÉu d√°ng', 
            'S·ªë ch·ªó', 'Tr·ªçng l∆∞·ª£ng', 'Tr·ªçng t·∫£i',
            'Lo·∫°i xe', 'Dung t√≠ch xe', 'Lo·∫°i ph·ª• t√πng', 'M√£ ph·ª• t√πng'
        ]
        
        existing_cols = [col for col in priority_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in priority_cols]
        df = df[existing_cols + other_cols]
        
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"\n{'='*70}")
        print(f"üíæ ƒê√É L∆ØU FILE")
        print(f"üìÅ {filename}")
        print(f"üìä {len(self.data)} s·∫£n ph·∫©m, {len(df.columns)} c·ªôt")
        print(f"üìã C√°c c·ªôt: {', '.join(df.columns.tolist())}")
        print(f"{'='*70}")
    
    def close(self):
        """ƒê√≥ng browser"""
        self.driver.quit()


def main():
    """H√†m ch√≠nh"""
    print("üöÄ Kh·ªüi ƒë·ªông Ch·ª£ T·ªët Scraper (FINAL VERSION)...")
    print("üìå Thay ƒë·ªïi:")
    print("   - L·∫•y gi√° t·ª´ class p26z2wb")
    print("   - CH·ªà l·∫•y section 'Th√¥ng s·ªë k·ªπ thu·∫≠t'")
    print("   - B·ªè qua 'T√¨nh tr·∫°ng xe' v√† c√°c section kh√°c")
    print("   - G·ªôp 'H√£ng' v√† 'H√£ng xe' th√†nh 1 c·ªôt 'H√£ng xe'")
    print("   - ∆Øu ti√™n n√∫t m≈©i t√™n ph·∫£i ƒë·ªÉ chuy·ªÉn trang\n")
    print()
    scraper = ChototScraper(headless=True)
    
    try:
        scraper.scrape_all_pages(max_products=100)
        scraper.export_to_excel('chotot_xemay_data.xlsx')
        
    except KeyboardInterrupt:
        print("\n\n‚ö† D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        if len(scraper.data) > 0:
            scraper.export_to_excel('chotot_partial.xlsx')
    except Exception as e:
        print(f"\n‚úó L·ªói: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nüîí ƒê√≥ng browser...")
        scraper.close()
        print("‚úÖ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()