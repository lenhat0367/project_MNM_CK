"""
Web Scraper cho Ch·ª£ T·ªët - C√†o d·ªØ li·ªáu √¥ t√¥ (DIRECT LINK VERSION)
Nh·∫≠n tr·ª±c ti·∫øp link danh s√°ch √¥ t√¥ v√† c√†o d·ªØ li·ªáu
"""

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class ChototScraper:
    def __init__(self, headless=True):
        """Kh·ªüi t·∫°o scraper"""
        gecko_path = "D:/project_MNM_CK/geckodriver.exe"
        firefox_path = "C:/Program Files/Mozilla Firefox/firefox.exe"
        
        self.options = Options()
        self.options.binary_location = firefox_path
        self.options.page_load_strategy = 'eager'
        
        if headless:
            self.options.add_argument('--headless')
        
        self.options.set_preference('permissions.default.image', 2)
        self.options.set_preference('general.useragent.override', 
                                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/120.0')
        
        service = Service(executable_path=gecko_path)
        self.driver = webdriver.Firefox(service=service, options=self.options)
        self.driver.set_page_load_timeout(20)
        self.wait = WebDriverWait(self.driver, 15)
        self.data = []
    
    def get_product_links_from_page(self):
        """L·∫•y t·∫•t c·∫£ links s·∫£n ph·∫©m t·ª´ trang hi·ªán t·∫°i"""
        product_links = []
        print(f"üì• ƒêang scroll ƒë·ªÉ load s·∫£n ph·∫©m...")
        
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scrolls = 10
        
        while scroll_attempts < max_scrolls:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts += 1
            if new_height == last_height:
                break
            last_height = new_height
        
        print(f"‚úì ƒê√£ scroll {scroll_attempts} l·∫ßn")
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            if href:
                if href.startswith('/'):
                    full_url = 'https://xe.chotot.com' + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                if ('chotot.com' in full_url and 
                    re.search(r'/\d+', full_url) and
                    '/mua-ban' in full_url and
                    '?' not in full_url and
                    full_url not in product_links):
                    product_links.append(full_url)
        
        if len(product_links) > 0:
            print(f"‚úì T√¨m th·∫•y {len(product_links)} s·∫£n ph·∫©m")
            print(f"üìù V√≠ d·ª• 3 links ƒë·∫ßu:")
            for i, link in enumerate(product_links[:3], 1):
                print(f"   {i}. {link}")
        
        return product_links
    
    def extract_specs(self, soup):
        """Tr√≠ch xu·∫•t C·∫¢ 2 ph·∫ßn: 'T√¨nh tr·∫°ng xe' V√Ä 'Th√¥ng s·ªë k·ªπ thu·∫≠t'"""
        specs = {}
        
        try:
            detail_section = soup.find('h2', class_='tfvqu6u', string=re.compile(r'Th√¥ng s·ªë'))
            
            if detail_section:
                main_container = detail_section.find_next('div', class_='pqop88r')
                
                if main_container:
                    all_sections = main_container.find_all('div', class_='befjs93')
                    
                    if all_sections:
                        for section in all_sections:
                            section_title = section.find('h3')
                            section_name = section_title.get_text(strip=True) if section_title else ""
                            
                            if section_name in ['T√¨nh tr·∫°ng xe', 'Th√¥ng s·ªë k·ªπ thu·∫≠t']:
                                spec_container = section.find('div', class_='s1cx459h')
                                if spec_container:
                                    spec_items = spec_container.find_all('div', class_='p1ja3eq0')
                                else:
                                    spec_items = section.find_all('div', class_=re.compile(r'p1ja3eq0'))
                                
                                for item in spec_items:
                                    all_spans = item.find_all('span', class_='bwq0cbs')
                                    
                                    if len(all_spans) >= 2:
                                        label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                        value = all_spans[1].get_text(strip=True)
                                        
                                        if label and value:
                                            specs[label] = value
                                    
                                    elif len(all_spans) == 1:
                                        label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                        link = item.find('a')
                                        if link:
                                            value_span = link.find('span', class_='bwq0cbs')
                                            if value_span:
                                                value = value_span.get_text(strip=True)
                                                if label and value:
                                                    specs[label] = value
                    
                    if not specs:
                        spec_container = main_container.find('div', class_='s1r2e0fc')
                        if spec_container:
                            spec_items = spec_container.find_all('div', class_=re.compile(r'pqp26ip|p1ja3eq0'))
                            
                            for item in spec_items:
                                all_spans = item.find_all('span', class_='bwq0cbs')
                                
                                if len(all_spans) >= 2:
                                    label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                    value = all_spans[1].get_text(strip=True)
                                    
                                    if label and value:
                                        specs[label] = value
                                
                                elif len(all_spans) == 1:
                                    label = all_spans[0].get_text(strip=True).replace(':', '').strip()
                                    link = item.find('a')
                                    if link:
                                        value_span = link.find('span', class_='bwq0cbs')
                                        if value_span:
                                            value = value_span.get_text(strip=True)
                                            if label and value:
                                                specs[label] = value
        
        except Exception as e:
            print(f"    ‚ö† L·ªói tr√≠ch xu·∫•t specs: {str(e)}")
        
        return specs
    
    def scrape_product(self, url):
        """C√†o th√¥ng tin chi ti·∫øt m·ªôt s·∫£n ph·∫©m"""
        try:
            self.driver.get(url)
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 800);")
            time.sleep(1)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            product_data = {
                'URL': url,
                'Ti√™u ƒë·ªÅ': '',
                'Gi√°': '',
            }
            
            title_elem = soup.find('h1')
            if title_elem:
                product_data['Ti√™u ƒë·ªÅ'] = title_elem.get_text(strip=True)
            
            price_elem = soup.find('b', class_='p26z2wb')
            if price_elem:
                product_data['Gi√°'] = price_elem.get_text(strip=True)
            else:
                price_patterns = [
                    soup.find(string=re.compile(r'\d+\.\d+\.\d+ ƒë')),
                    soup.find(string=re.compile(r'\d+ tri·ªáu')),
                    soup.find(string=re.compile(r'\d+\.\d+ t·ª∑')),
                ]
                for price_elem in price_patterns:
                    if price_elem:
                        product_data['Gi√°'] = price_elem.strip()
                        break
            
            specs = self.extract_specs(soup)
            product_data.update(specs)
            
            if 'H√£ng' in product_data and 'H√£ng xe' not in product_data:
                product_data['H√£ng xe'] = product_data.pop('H√£ng')
            elif 'H√£ng' in product_data and 'H√£ng xe' in product_data:
                product_data.pop('H√£ng')
            
            return product_data
            
        except Exception as e:
            print(f"    ‚úó L·ªói c√†o s·∫£n ph·∫©m: {str(e)}")
            return None
    
    def go_to_next_page(self, current_page):
        """Chuy·ªÉn sang trang ti·∫øp theo - ∆∞u ti√™n n√∫t m≈©i t√™n ph·∫£i"""
        next_page = current_page + 1
        
        try:
            right_arrow_button = self.driver.find_element(
                By.XPATH,
                "//button[@class='Paging_redirectPageBtn__KvsqJ' and .//i[contains(@class, 'rightIcon') and not(contains(@class, 'Disable'))]]"
            )
            
            print(f"\n‚û°Ô∏è  T√¨m th·∫•y n√∫t m≈©i t√™n ph·∫£i, ƒëang click...")
            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", right_arrow_button)
            time.sleep(1)
            right_arrow_button.click()
            time.sleep(4)
            
            print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
            print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
            return True
            
        except Exception as e1:
            print(f"   ‚ö† Kh√¥ng t√¨m th·∫•y n√∫t m≈©i t√™n ph·∫£i")
            
            try:
                next_page_link = self.driver.find_element(
                    By.XPATH, 
                    f"//a[@href='/mua-ban-oto-tp-ho-chi-minh?page={next_page}']"
                )
                
                print(f"\n‚û°Ô∏è  T√¨m th·∫•y link trang {next_page}, ƒëang click...")
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_page_link)
                time.sleep(1)
                next_page_link.click()
                time.sleep(4)
                
                print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
                print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                return True
                
            except Exception as e2:
                print(f"   ‚ö† Kh√¥ng t√¨m th·∫•y link trang {next_page}")
                
                try:
                    new_url = f"https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh?page={next_page}"
                    print(f"\n‚û°Ô∏è  Thay ƒë·ªïi URL tr·ª±c ti·∫øp sang trang {next_page}...")
                    self.driver.get(new_url)
                    time.sleep(4)
                    
                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    product_pattern = re.compile(r'/mua-ban.*\d+')
                    products = soup.find_all('a', href=product_pattern)
                    
                    if len(products) > 0:
                        print(f"‚úì ƒê√£ chuy·ªÉn sang trang {next_page}")
                        print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
                        return True
                    else:
                        print(f"‚úó Trang {next_page} kh√¥ng c√≥ s·∫£n ph·∫©m, ƒë√£ h·∫øt d·ªØ li·ªáu")
                        return False
                    
                except Exception as e3:
                    print(f"\n‚úó Kh√¥ng th·ªÉ chuy·ªÉn trang: {str(e3)}")
                    return False
    
    def scrape_from_url(self, start_url, max_products=100):
        """C√†o d·ªØ li·ªáu t·ª´ URL ƒë∆∞·ª£c cung c·∫•p tr·ª±c ti·∫øp"""
        print("=" * 70)
        print("B·∫ÆT ƒê·∫¶U C√ÄO D·ªÆ LI·ªÜU T·ª™ URL")
        print("=" * 70)
        print(f"üîó URL: {start_url}\n")
        
        # Truy c·∫≠p tr·ª±c ti·∫øp URL
        print("üìå ƒêang truy c·∫≠p trang danh s√°ch √¥ t√¥...")
        self.driver.get(start_url)
        time.sleep(3)
        print(f"‚úì ƒê√£ v√†o trang: {self.driver.title}")
        print(f"‚úì URL hi·ªán t·∫°i: {self.driver.current_url}")
        
        print("\n" + "=" * 70)
        print("B·∫ÆT ƒê·∫¶U C√ÄO S·∫¢N PH·∫®M √î T√î")
        print("=" * 70)
        
        page_num = 1
        
        while len(self.data) < max_products:
            print(f"\n{'='*70}")
            print(f"üìÑ TRANG {page_num}")
            print(f"üîó URL: {self.driver.current_url}")
            print(f"{'='*70}")
            
            product_links = self.get_product_links_from_page()
            
            if len(product_links) == 0:
                print("‚ö† Kh√¥ng c√≤n s·∫£n ph·∫©m, d·ª´ng l·∫°i")
                break
            
            remaining = max_products - len(self.data)
            product_links = product_links[:remaining]
            
            print(f"\nüîÑ C√†o {len(product_links)} s·∫£n ph·∫©m t·ª´ trang n√†y...")
            
            for i, link in enumerate(product_links, 1):
                print(f"\n  [{page_num}.{i}/{len(product_links)}] ", end="")
                product_data = self.scrape_product(link)
                
                if product_data and product_data.get('Ti√™u ƒë·ªÅ'):
                    self.data.append(product_data)
                    print(f"‚úì {product_data.get('Ti√™u ƒë·ªÅ', '')[:50]}")
                    print(f"    üí∞ {product_data.get('Gi√°', 'N/A')}")
                    
                    spec_count = len([k for k in product_data.keys() if k not in ['URL', 'Ti√™u ƒë·ªÅ', 'Gi√°']])
                    if spec_count > 0:
                        print(f"    üìä {spec_count} th√¥ng s·ªë")
                else:
                    print(f"‚úó L·ªói")
                
                time.sleep(0.5)
            
            print(f"\n‚úì Ho√†n th√†nh trang {page_num}")
            print(f"üìä T·ªïng ƒë√£ c√†o: {len(self.data)}/{max_products}")
            
            if len(self.data) >= max_products:
                print("\nüéØ ƒê√£ ƒë·ªß s·ªë l∆∞·ª£ng s·∫£n ph·∫©m c·∫ßn c√†o")
                break
            
            if not self.go_to_next_page(page_num):
                print("\n‚ö† Kh√¥ng th·ªÉ chuy·ªÉn trang, d·ª´ng l·∫°i")
                break
            
            page_num += 1
        
        print(f"\n{'='*70}")
        print(f"üéâ HO√ÄN T·∫§T: ƒê√£ c√†o {len(self.data)} s·∫£n ph·∫©m")
        print(f"{'='*70}")
    
    def export_to_excel(self, filename='chotot_oto_data.xlsx'):
        """Xu·∫•t d·ªØ li·ªáu ra Excel"""
        if not self.data:
            print("\n‚úó Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t!")
            return
        
        df = pd.DataFrame(self.data)
        
        priority_cols = [
            'URL', 'Ti√™u ƒë·ªÅ', 'Gi√°',
            'H√£ng', 'H√£ng xe', 'D√≤ng xe', 'NƒÉm s·∫£n xu·∫•t', 'Phi√™n b·∫£n xe',
            'H·ªôp s·ªë', 'Nhi√™n li·ªáu', 'Ki·ªÉu d√°ng', 
            'S·ªë ch·ªó', 'S·ªë c·ª≠a', 'Tr·ªçng l∆∞·ª£ng', 'Tr·ªçng t·∫£i',
            'Lo·∫°i xe', 'Dung t√≠ch xe',
            'S·ªë Km ƒë√£ ƒëi', 'S·ªë ƒë·ªùi ch·ªß', 'T√¨nh tr·∫°ng', 'Xu·∫•t x·ª©',
            'C√≥ ph·ª• ki·ªán ƒëi k√®m', 'C√≤n h·∫°n ƒëƒÉng ki·ªÉm', 'Ch√≠nh s√°ch b·∫£o h√†nh',
            'Lo·∫°i ph·ª• t√πng', 'M√£ ph·ª• t√πng'
        ]
        
        existing_cols = [col for col in priority_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in priority_cols]
        df = df[existing_cols + other_cols]
        
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"\n{'='*70}")
        print(f"üíæ ƒê√É L∆ØU FILE")
        print(f"üìÅ {filename}")
        print(f"üìä {len(self.data)} s·∫£n ph·∫©m, {len(df.columns)} c·ªôt")
        print(f"üìã C√°c c·ªôt: {', '.join(df.columns.tolist())}")
        print(f"{'='*70}")
    
    def close(self):
        """ƒê√≥ng browser"""
        self.driver.quit()


def main():
    """H√†m ch√≠nh"""
    print("üöÄ Kh·ªüi ƒë·ªông Ch·ª£ T·ªët Scraper - C√ÄO √î T√î (DIRECT LINK)")
    print("=" * 70)
    print("üìå T√≠nh nƒÉng:")
    print("   ‚úì Nh·∫≠n tr·ª±c ti·∫øp link danh s√°ch √¥ t√¥")
    print("   ‚úì L·∫•y gi√° t·ª´ class p26z2wb")
    print("   ‚úì L·∫•y c·∫£ 'Th√¥ng s·ªë k·ªπ thu·∫≠t' v√† 'T√¨nh tr·∫°ng xe'")
    print("   ‚úì G·ªôp 'H√£ng' v√† 'H√£ng xe' th√†nh 1 c·ªôt")
    print("   ‚úì ∆Øu ti√™n n√∫t m≈©i t√™n ph·∫£i ƒë·ªÉ chuy·ªÉn trang")
    print("=" * 70)
    print()
    
    # URL m·∫∑c ƒë·ªãnh - c√≥ th·ªÉ thay ƒë·ªïi
    url = "https://xe.chotot.com/mua-ban-oto-tp-ho-chi-minh"
    
    scraper = ChototScraper(headless=True)
    
    try:
        scraper.scrape_from_url(url, max_products=100)
        scraper.export_to_excel('chotot_oto_data.xlsx')
        
    except KeyboardInterrupt:
        print("\n\n‚ö† D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        if len(scraper.data) > 0:
            scraper.export_to_excel('chotot_oto_partial.xlsx')
    except Exception as e:
        print(f"\n‚úó L·ªói: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nüîí ƒê√≥ng browser...")
        scraper.close()
        print("‚úÖ Ho√†n t·∫•t!")


if __name__ == "__main__":
    main()